---
title: "Raport 2"
subtitle: "Eksploracja danych"
author:   |
          |    Mikołaj Langner, Marcin Kostrzewa
          |    nr albumów: 255716, 255749
date: "2021-04-19"
output: 
  pdf_document:
    toc: true
    fig_caption: yes
    fig_width: 5 
    fig_height: 4 
    number_sections: true
    includes:
      in_header: "preambula.tex" 
fontsize: 12pt 
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4, fig.pos='H')
library(datasets)
library(dplyr)
library(MASS)
library(cluster)
library(DataExplorer)
library(ggplot2)
library(GGally)
library(ggbeeswarm)
library(ggfortify)
library(ggbiplot)
library(ggcorrplot)
library(wesanderson)
library(cowplot)
library(EnvStats)
library(arules)
library(e1071)
library(kableExtra)
library(reshape2)
```

# Wstęp

Sprawozdanie zawiera rozwiązanie zadań z listy 2. 

Zadanie pierwsze dotyczy pojęcia dyskretyzacji i badania jego jakości.

Zadanie drugie i trzecie dotykają pojęcia metod redukcji wymiaru: 

- w zadaniu drugim skorzystamy z metody składowych głównych,
- w zadaniu trzecim z metody skalowania wielowymiarowego.

\newpage

# Zadanie 1

W pierwszym zadaniu mamy dokonać dyskretyzacji cech ciągłych ze zbioru `iris` i ocenić jej jakość.

## Wczytanie danych i wstępna analiza

```{r}
data(iris)
```

Wybierzmy zmienne o najlepszej i najgorszej zdolności dyskryminacyjnej. W tym celu narysujemy wykresy pudełkowe oraz wyliczymy współczynniki zmienności każdej ze zmiennych z podziałem na poszczególne gatunki irysów i porównamy ich rozkłady.

```{r include=FALSE}
str(iris)
plot_intro(iris)
```

```{r iris boxplot, fig.cap="Wykresy pudelkowe dla zmiennych ze zbioru iris", fig.height=3, fig.width=5}
plot_boxplot(iris, by="Species")
```

```{r, echo=FALSE}
table <- aggregate(. ~ Species, iris, cv)
table %>% kbl(caption="Wspolczynniki zmiennosci dla poszczegolnych zmiennych", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position'))
```

Możemy zauważyć, że zmienna Petal.Length najefektywniej rozdziela poszczególne gatunki, natomiast zmienna Sepal.Width radzi sobie z tym najgorzej.

## Metody dyskretyzacji

Porównamy ze sobą cztery metody dyskretyzacji nienadzorowanej:

 - equal width,
 - equal frequency,
 - k-means clustering,
 - dyskretyzację dla przedziałów zadanych przez użytkownika.

### Najlepiej separująca zmienna

Zacznijmy od zmiennej Petal.Length, która najlepiej rozdziela poszczególne gatunki irysów.


```{r message=FALSE, fig.height=3, fig.width=6, echo=FALSE}
intervals <- c(min(iris$Petal.Length), 2, 5, max(iris$Petal.Length))

for (method in c("interval", "frequency", "cluster", "fixed")) {
  petal.length.discretized <- if (method != "fixed") 
    discretize(iris$Petal.Length, method=method) else 
    discretize(iris$Petal.Length, method=method, breaks=intervals)
  print(plot_grid(
    ggplot(iris, aes(Petal.Length)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(petal.length.discretized)$"discretized:breaks") +
          ggtitle(method),
    ggplot(iris, aes(Species, Petal.Length)) +
      geom_quasirandom(aes(col=Species)) +
      scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
      geom_hline(yintercept=attributes(petal.length.discretized)$"discretized:breaks") +
      theme(legend.position = "none")))
  discretized.table <- table(petal.length.discretized, iris$Species)
  matchClasses(discretized.table)
}
```

### Najgorzej separująca zmienna

Możemy zobaczyć teraz jak poszczególne metody działają dla zmiennej Sepal.Width, która najgorzej radzi sobie z rozdzielaniem gatunków.

```{r message=FALSE, fig.height=3, fig.width=3.25, echo=FALSE}
intervals <- c(min(iris$Sepal.Width), 2.5, 3, max(iris$Sepal.Width))
for (method in c("interval", "frequency", "cluster", "fixed")) {
  sepal.width.discretized <- if (method != "fixed") 
    discretize(iris$Sepal.Width, method=method) else 
    discretize(iris$Sepal.Width, method=method, breaks=intervals)
  print(ggplot(iris, aes(Sepal.Width)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
          ggtitle(method))
  print(ggplot(iris, aes(Species, Sepal.Width)) +
    geom_quasirandom(aes(col=Species)) +
    scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
    geom_hline(yintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
    theme(legend.position = "none"))
  discretized.table <- table(sepal.width.discretized, iris$Species)
  matchClasses(discretized.table)
}
```

Dla obu zmiennych każda z metod wypada równie dobrze, przy czym, najlepsze wyniki dają metody równej częstości oraz k-średnich.


## Metody dyskretyzacji z wartościami odstającymi

Rozpatrzmy teraz dyskretyzację przy dodaniu sztucznie wartości odstających.

### Zmienna Petal.Length

Zacznijmy znowu od zmiennej Petal.Length.

```{r message=FALSE, fig.height=3, fig.width=3.25, echo=FALSE}
iris$Petal.Length[which.min(iris$Petal.Length)] <- min(iris$Petal.Length) - IQR(iris$Petal.Length)
iris$Petal.Length[which.max(iris$Petal.Length)] <- max(iris$Petal.Length) + IQR(iris$Petal.Length)
intervals <- c(min(iris$Petal.Length), 2, 5, max(iris$Petal.Length))
for (method in c("interval", "frequency", "cluster", "fixed")) {
  petal.length.discretized <- if (method != "fixed") 
    discretize(iris$Petal.Length, method=method) else 
    discretize(iris$Petal.Length, method=method, breaks=intervals)
  print(ggplot(iris, aes(Petal.Length)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(petal.length.discretized)$"discretized:breaks") +
          ggtitle(method))
  print(ggplot(iris, aes(Species, Petal.Length)) +
    geom_quasirandom(aes(col=Species)) +
    scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
    geom_hline(yintercept=attributes(petal.length.discretized)$"discretized:breaks") +
    theme(legend.position = "none"))
  discretized.table <- table(petal.length.discretized, iris$Species)
  matchClasses(discretized.table)
}
```

### Zmienna Sepal.Width

Dla zmiennej Sepal.Width po dodaniu wartości odstających dyskretyzacja wygląda następująco:

```{r message=FALSE, fig.height=3, fig.width=3.25, echo=FALSE}
iris$Sepal.Width[which.min(iris$Sepal.Width)] <- min(iris$Sepal.Width) - IQR(iris$Sepal.Width)
iris$Sepal.Width[which.max(iris$Sepal.Width)] <- max(iris$Sepal.Width) + IQR(iris$Sepal.Width)
intervals <- c(min(iris$Sepal.Width), 2.5, 3, max(iris$Sepal.Width))
for (method in c("interval", "frequency", "cluster", "fixed")) {
  sepal.width.discretized <- if (method != "fixed") 
    discretize(iris$Sepal.Width, method=method) else 
    discretize(iris$Sepal.Width, method=method, breaks=intervals)
  print(ggplot(iris, aes(Sepal.Width)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
          ggtitle(method))
  print(ggplot(iris, aes(Species, Sepal.Width)) +
    geom_quasirandom(aes(col=Species)) +
    scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
    geom_hline(yintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
    theme(legend.position = "none"), caption="test")
  discretized.table <- table(sepal.width.discretized, iris$Species)
  matchClasses(discretized.table)
}
```

Nie powinien dziwić fakt, że największa zmiana w poprawności predykcji dotknęła metodę przedziałów równej długości, gdyż pojedyncza obserwacja całkowicie zmienia dobór miejsc partycji przedziału.

\newpage

# Zadanie 2

## Wczytanie i przygotowanie danych

Teraz naszym zadaniem jest dokonanie analizy składów głównych (PCA) dla zbioru `state.x77`, który zawiera informacje o wskaźnikach terytorialno-społecznych dla wszystkich amerykańskich stanów.

Najpierw wczytajmy dane i uzupełnijmy je o informacje geograficzne.

```{r state dataset}
data(state)
state <- as.data.frame(state.x77)
state$region <- state.region
state$division <- state.division
state.subset <- subset(state, select=-c(region, division))
```

By rozstrzygnąć, czy potrzebna jest normalizacja danych, przeanalizujemy wykresy pudełkowe oraz wyznaczymy odchylenia standardowe i współczynniki zmienności.

```{r data boxplot, message=FALSE, echo=FALSE, fig.height=2.8, fig.cap="Wykresy pudelkowe dla zmiennych ze zbioru state.x77", fig.pos='!H'}
ggplot(data=melt(state.subset), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable)) + theme(legend.position = "none") + 
  labs(x = "Zmienne", y = "Wartosc")
```

```{r tabela_1, echo=FALSE, eval=TRUE, results='asis'}
var <- sapply(state.subset, function(X) c(sd(X), cv(X)))
summary.matrix <- as.matrix(var)
row.names(summary.matrix) <- c("Odchylenie standardowe", "Wspolczynnik zmiennosci")
summary.matrix %>% kbl(caption="Odchylenie standardowe i wspolczynnik zmiennosci dla zmienych", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position', 'scale_down'))
```

Widać, że zmienne wymagają standaryzacji --- ich wariancje zbyt mocno się różnią.

## Składowe główne i ich analiza

Wyznaczymy teraz składowe główne i przedstawimy ich rozrzut, wykorzystując wykresy pudełkowe.

```{r pca}
after.pca <- prcomp(state.subset, retx = T, center = T, scale. = T)
```

```{r pca boxplot, echo=FALSE, message=FALSE, fig.height=2.9, fig.cap="Wykresy pudelkowe dla skladowych glownych"}
ggplot(data = melt(data.frame(after.pca$x)), 
       aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable)) + 
      theme(legend.position = "none") + labs(x = "Skladowe glowne", y = 'Wartosc')
```

Przypatrzmy się teraz wektorom ładunków dla trzech pierwszych składowych głównych.

```{r loadings table, echo=FALSE}
matrix = as.matrix(after.pca$rotation[, c(1:3)])
matrix %>% kbl(caption="Wektory ladunkow dla trzech pierwszych PC", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position'))
```

- W przypadku pierwszej składowej głównej, największy wkład mają zmienne `Illiteracy`, `Murder`, `HS Grad` i `Life Exp`. Dwie pierwsze mają ten sam znak, możemy więc wnioskować, że są ze sobą powiązane. `HS Grad` i `Life Exp` mają znak przeciwny --- stąd te dwie pary są ze sobą negatywnie skorelowane. Jest to dość oczywisty rodzaj zależności między stopniem analfabetyzmu a procentem ilości mających ukończoną szkołę średnią i między ilością morderstw a średnią długością życia.

- W przypadku drugiej składowej głównej, największą wagę mają zmienne `Area`, `Population` i `Income`. Zależność między `Area` a `Population` jest dość oczywista, natomiast zależność tych zmiennych od `Income` już niekoniecznie da się łatwo wytłumaczyć. 

Zbadajmy teraz jaka część wyjaśnionej wariancji odpowiada kolejnym składowym głównym.


```{r variance, echo=FALSE, fig.width=8, fig.cap="Wariancja wyjasniana przez poszczegolne skladowe glowne i wariancja skumulowana."}
variance_proportion <- (after.pca$sdev ^2) / sum(after.pca$sdev^2)

df <- data.frame(variance_proportion)

df["cumulative.variance"] <- cumsum(variance_proportion)

labels <- paste0("PC", 1:8)
df["PC"] = labels

barplot1 <- ggplot(data=df,
                   aes(x=PC, y=variance_proportion, fill=PC)) + 
                   geom_bar(stat="identity") + scale_fill_brewer(palette="Dark2") +
                   theme(legend.position = "none")
barplot2 <- ggplot(data=df, aes(x=PC, y=cumulative.variance, fill=PC)) + 
                    geom_bar(stat="identity") + scale_fill_brewer(palette="Dark2") +
                    theme(legend.position = "none")
plot_grid(barplot1, barplot2, ncol=2, nrow=1)
```


```{r table, echo=FALSE}
variance.matrix <- t(df[1:5, 1:2])
colnames(variance.matrix) <- paste0("PC", 1:5)
rownames(variance.matrix) <- c("Proporcja wariancji", "Skumulowana wariancja")
variance.matrix %>% kbl(caption="Procent wyjasnianej wariancji i skumulowana wariancja", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position'))
```


Zauważamy, że:

- PC1 wyjaśnia $45 \%$ wyjaśnianej wariancji, PC2 prawie $25 \%$;
- $80 \%$ całkowitej wariancji jest wyjaśniane przez pierwsze cztery składowe główne (trzy pierwsze wyjaśniają niewiele mniej), $90 \%$ jest wyjaśniane zaś przez pierwszych 5.


## Wizualizacja danych

W tej części wygenerujemy dwuwymiarowe wykresy rozrzutu (\ref{fig:2d_scatterplot}) dla dwóch pierwszych składowych głównych. Skorzystamy z danych dotyczących lokalizacji poszczególnych stanów, by być w stanie wyciągnąc interesujące wnioski.


```{r 2d_scatterplot, echo=FALSE, fig.width=10, fig.height=4, fig.cap="\\label{fig:2d_scatterplot}Wykresy rozrzutu"}
p1 <- ggplot(data = data.frame(after.pca$x), aes(x=PC1, y=PC2)) + 
        geom_text(aes(color = state$region, label = rownames(state)), size = 2, nudge_y = 0.2) + 
        geom_point(aes(color= state$region))

p2 <- ggplot(data = data.frame(after.pca$x), aes(x=PC1, y=PC2)) + 
        geom_text(aes(color = state$division, label = rownames(state)), size = 2, nudge_y = 0.2) + 
        geom_point(aes(color = state$division))

plot_grid(p1, p2, nrow = 1, ncol = 2)
```


Obserwacje:

- Stany zlokalizaowane w południowych częściach USA są stosunkowo blisko względem siebie położone --- możemy więc wnioskować o ich dużym podobieństwie. Są one też często dość oddalone od pozostałych obserwacji. 


```{r Alaska and California stats, echo=FALSE, results='hide'}
rownames(state)[which.max(state$Population)]
rownames(state)[which.max(state$Income)]
rownames(state)[which.max(state$Area)]
```

- Są dwie obseracje, które znacząco różnią się od pozostałych. Są to Alaska i Kalifornia. Alaska jest stanem o największej powierzchni, dochód na jednego mieszkańca jest tam również najwyższy. Natomiast w Kalifornii mieszka najwięcej ludzi (stan ten charakteryzuje się także dużą powierzchnią).


Przygotowaliśmy także wykresy 3d --- kod umieściliśmy w dodatkowym skrypcie. 

## Korelacja zmiennych

Zbadamy teraz korelację między zmiennymi. Najpierw skorzystamy z dwuwykresu.

```{r biplot, echo=FALSE, fig.cap="Dwuwykres dla danych state.x77", fig.width=6, fig.height=3}
ggbiplot(after.pca, obs.scale = 2, var.scale = 2, varname.size = 3.7, ellipse = TRUE, 
         groups = state$region)
```

Możemy zaobserwować, że zmienne `Murder` i `Illiteracy` są ze dodatnio i silnie skorelowane. Podobnie zachowują się zmienne `Income` i `HS Grad`. Ujemna korelacja jest możliwa do zaobserwowania pomiędzy zmiennymi `Life Exp` i `Murder`, `HS Grad` i `Illiteracy`. Zmienna `Frost` jest ujemnie skorelowane z `Illiteracy` i `Murder`.

Te wnioski potwierdzają się, jeżeli popatrzymy na mapę ciepła korelacji zmiennych.

```{r, echo=FALSE, fig.cap="Mapa ciepla korelacji zmiennych"}
cor.matrix <- cor(state.subset)
ggcorrplot(cor.matrix, lab = TRUE)
```

## Wnioski do zadania 2

Dzięki zastosowaniu metody analizy składowych głównych udało się nam otrzymać ciekawe wnioski dotyczące stanów USA.

Przede wszystkim, stany zlokalizowane na południu kraju są bardzo do siebie podobne. Charakteryzują się największym stopniem analfabetyzmu, największą ilością morderstw, najniższym stopniem wykształcenia. Są to też na ogół cieplejsze stany, co tłumaczy ujemną korelację pomiędzy zmiennymi `Murder`, `Illiteracy` a `Frost`.

Inne stany (poza dwoma wyjątkami --- Alaską i Kalifornią) znajdowały się dość blisko siebie. Stąd trudno o ogólne wnioski na temat znaczących różnic pomiędzy nimi.

\newpage

# Zadanie 3

## Wybrany zbiór danych

Wybranym przez nas zbiorem danych jest `Stars`, który dostępny jest na Kaggle'u ([link](https://www.kaggle.com/brsdincer/star-type-classification)). Zawiera on 240 obserwacji i 7 następujących zmiennych:

- `Temperature` --- temperatura gwiazdy wyrażona w Kelwinach (ilościowa),

- `L` --- stosunek jasności gwiazdy i jasności Słońca (ilościowa),

- `R` --- stosunek promienia gwiazdy do promienia Słońca (ilościowa),

- `A_M` --- absolutna wielkość gwiazdowa (ilościowa),

- `Color` --- kolor gwiazdy (jakościowa),

- `Spectral_Class` --- typ spektralny gwiazdy (jakościowa),

- `Type` --- typ gwiazdy --- jedno z: czerwony, biały, brązowy karzeł, gwiazda ciągu głównego, nadolbrzym, hiperolbrzym (jakościowa).

Zmienną `Type` będziemy traktować jako zmienną grupującą.

Wczytamy teraz dane (z całego zbioru wybieramy losowo 100 obserwacji), przygotujemy je do do skalowania wielowymiarowego i wyznaczymy macierz niepodobieństwa.

```{r stats_dataset}
set.seed(42)
stars <- sample_n(read.csv("Stars.csv"), 100)
```

```{r mds data, echo=FALSE}
stars[, c("Color", "Type", "Spectral_Class")] = 
  lapply(stars[, c("Color", "Type", "Spectral_Class")], as.factor)
levels(stars$Type) <- list("Red Dwarf" = '0', 'Brown Dwarf' = '1', 
                           'White Dwarf' = '2', 'Main Sequence' = '3',
                           'Super Giants' = '4', 'Hyper Giants' = '5')
data.for.mds <- subset(stars, select=-Type)
dist.matrix <- as.matrix(daisy(data.for.mds, stand = T))
```

Przy wyznaczaniu odległości między obserwacjami za pomocą funkcji `daisy` dane poddajemy standaryzacji. O konieczności tej operacji świadczą wykresy pudełkowe zmiennych ciągłych. 

```{r mds boxplot, message=FALSE, echo=FALSE, fig.height=3, fig.cap="Wykresy pudelkowe dla zmiennych ciaglych ze zbioru Stars", fig.pos='!H'}
ggplot(data=melt(subset(data.for.mds, select=-c(Color, Spectral_Class))), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable)) +
                   theme(legend.position = "none") + labs(x = "Zmienne", y = "Wartosc")
```

Poniżej także wygenerowaliśmy mapę ciepła macierzy niepodobieństwa. 

```{r heatmap, echo=FALSE, fig.height=4, fig.width=4, fig.cap="\\label{fig::heatmap}Mapa ciepla macierzy niepodobienstwa"}
image(x = dist.matrix, axes = FALSE)
```

## Redukcja wymiaru na bazie MDS i analiza jej jakości

Porównamy teraz jakoś odwzorowania MDS w zależności od wielkości wymiaru $d$ przestrzeni docelowej. Przedstawimy na wykresie wartości funkcji STRESS, jak i wykonamy diagramy Sheparda. Porównamy ze sobą skalowanie klasyczne (funkcja `cmdscale`) i niemetryczne --- wykorzystujące metrykę Kruskala-Sheparda (funkcja `isoMDS`).


```{r stress function and preparing data for shepard}
d.max <- 6

data.for.shepard <- data.frame('original' = as.vector(dist.matrix))
STRESS.values <- numeric(d.max)

for (i in 1:d.max) {
  mds.k <- as.matrix(cmdscale(dist.matrix, k = i))
  new.dist.matrix <- as.matrix(dist(mds.k, method = "euclidean"))
  
  STRESS.values[i] <- sum((new.dist.matrix - dist.matrix)^2)
  
  data.for.shepard[paste0("d=", i)] = as.vector(new.dist.matrix)
}
```


Poniżej znajduje się wykres funkcji STRESS dla skalowania klasycznego i odpowiadające mu diagramy Sheparda (\ref{fig::shepard}).

```{r STRESS function plot, echo=FALSE, fig.height=4, fig.width=5, fig.cap="Wykres funkcji STRESS dla skalowania klasycznego"}
ggplot(data = data.frame(STRESS.values), aes(x=seq_along(STRESS.values), y=STRESS.values)) + 
        geom_point(color = wes_palette("GrandBudapest1", 3)[2]) + 
        geom_line(color = wes_palette("GrandBudapest1", 3)[3]) + labs(x = "wymiar", y = "Wartosci funkcji STRESS")
```

```{r shepard diagram, echo=FALSE, fig.cap="\\label{fig::shepard}Diagramy Sheparda dla kolejnych wymiarow"}
data.1 <- melt(data.for.shepard, id.vars = "original")
ggplot(data = data.1, aes(x=original, y=value)) + geom_point() + geom_abline(slope = 1, color = "red")+ facet_wrap(~variable, scales = "free") + labs(x = "odleglosci w danej przestrzeni", y = "odleglosci w pierwotnej przestrzeni")
```


Przyglądając się diagramom Sheparda, możemy dojść do wniosku, że klasyczne MDS nie działa dla naszych danych idealnie. Świadczy o tym również wartość funkcji STRESS dla maksymalnego wymiaru $\text{d.max} = 6$ --- `r round(STRESS.values[6], 2)`. Na podstawie tych wykresów możemy też stwierdzić, że optymalnym wyborem wymiaru byłoby $d = 4$, dla którego wartość funkcji stresu wynosi `r round(STRESS.values[4], 2)` --- niewiele mniej niż dla $d.max$.


Teraz powtórzymy te czynności dla skalowania Kruskala-Sheparda --- będą to wykresy (\ref{fig::k_stress}) i (\ref{fig::shep_krus}).

```{r kruskal_shepard}
kruskal.stress <- numeric(d.max)
data.for.shepard.2 <- data.frame('original' = as.vector(dist.matrix))

for (i in 1:d.max) {
  mds.k <- isoMDS(dist.matrix, k = i, trace = FALSE)
  new.dist.matrix <- as.matrix(dist(mds.k$points, method = "euclidean"))
  
  kruskal.stress[i] <- mds.k$stress
  
  data.for.shepard.2[paste0("d=", i)] = as.vector(new.dist.matrix)
}
```



```{r STRESS_kruskal_plot, echo=FALSE, fig.height=4, fig.width=5, fig.cap="\\label{fig::k_stress}Wykres funkcji STRESS dla skalowania Kruskala"}
ggplot(data = data.frame(kruskal.stress), aes(x=seq_along(kruskal.stress), y=kruskal.stress)) + 
        geom_point(color = wes_palette("GrandBudapest1", 3)[2]) + 
        geom_line(color = wes_palette("GrandBudapest1", 3)[3]) + labs(x = "wymiar", y = "Wartosci funkcji STRESS")
```



```{r shepard_kruskal_diagram, echo=FALSE, fig.cap="\\label{fig::shep_krus}Diagramy Sheparda dla kolejnych wymiarow dla skalowania Kruskala"}
data.2 <- melt(data.for.shepard.2, id.vars = "original")
ggplot(data = data.2, aes(x=original, y=value)) + geom_point() + geom_abline(slope = 1, color = "red")+ facet_wrap(~variable, scales = "free") + labs(x = "odleglosci w danej przestrzeni", y = "odleglosci w pierwotnej przestrzeni")
```

Możemy zauważyć, że funkcja STRESS przyjmuje mniejsze wartości dla tego rodzaju skalowania. Jednak zmiany tej funkcji dla skalowania Kruskala są bardziej gładkie --- optymalnym wymiarem docelowym również byłoby $d = 4$.

## Wizualizacja danych

Wykorzystamy teraz metodę skalowania wielowymiarowego do przedstawienia naszych danych w przestrzeni dwuwymiarowej
(przygotowaliśmy także wykresy w przestrzeni trójwymiarowej --- znajdują się one w dołączonym do sprawozdania skrypcie).


```{r 2d mds plot2, echo=FALSE, fig.cap="Wykres rozrzutu w przestrzeni dwuwymiarowej --- skalowanie klasyczne"}
mds.2 <- cmdscale(dist.matrix, k = 2)

data.2 <- data.frame(mds.2)
ggplot(data = data.2, aes(x=X1, y=X2)) + 
       geom_point(aes(color = stars$Type))  + labs(x = "MD1", y = "MD2")
```

```{r 2d_mds_kruskal, echo=FALSE, fig.cap="Wykres rozrzutu w przestrzeni dwuwymiarowej --- skalowanie Kruskala"}
mds.22 <- isoMDS(dist.matrix, k = 2, trace = FALSE)

data.22 <- data.frame(mds.22$points)
ggplot(data = data.22, aes(x=X1, y=X2)) + 
       geom_point(aes(color = stars$Type))  + labs(x = "MD1", y = "MD2")
```


Dla obu metod widać wyraźny podział na klastry, które odpowiadają prawdziwym typom gwiazd. Podział jest tak wyraźny mimo naszych wczesniejszych wniosków dotyczących jakości skalowania klasycznego. Ciekawą obserwacją jest bardzo bliskie położenie czerwonych i brązowych karłów, które wręcz na siebie nachodzą. Odpowiada to rzeczzywistości --- brązowe i czerwone karły to obiekty o podobnych własnościach (np. niska temperatura).


Na obu wykresach rozrzutu można zauważyć obserwację odstającą --- jest to jedna z gwiazd z ciągu głównego. Znajduje się ona na wykresach w klastrze odpowiadającym nadolbrzymom.


```{r Wykresy rozrzutu, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label{fig::scatter_end}Wykresy rozrzutu dla zmiennych ciaglych"}
subset <- subset(data.for.mds, select=-c(Color, Spectral_Class))
ggpairs(subset,
mapping = ggplot2::aes(color = as.factor(stars$Type)),
columns=1:3, 
lower=list(continuous=wrap("points", alpha=.4, size=.01)),
upper=list(continuous="blank"),
diag=list(continuous="blank"))
```

Zastosowanie MDS umożliwiło nam znacznie wartościowszą analizę wybranych przez nas danych. Na wykresach rozrzutu (\ref{fig::scatter_end}) nie widać tak dobrego podziału na klastry jak przy zastosowaniu metody skalowania wielowymiarowego. 


