---
title: "Raport 2"
subtitle: "Eksploracja danych"
author:   |
          |    Mikołaj Langner, Marcin Kostrzewa
          |    nr albumów: 255716, 255749
date: "2021-04-19"
output: 
  pdf_document:
    toc: true
    fig_caption: yes
    fig_width: 5 
    fig_height: 4 
    number_sections: true
    includes:
      in_header: "preambula.tex" 
fontsize: 12pt 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4)
library(datasets)
library(dplyr)
library(MASS)
library(cluster)
library(DataExplorer)
library(ggplot2)
library(ggbeeswarm)
library(ggfortify)
library(ggbiplot)
library(ggcorrplot)
library(wesanderson)
library(cowplot)
library(EnvStats)
library(arules)
library(e1071)
library(kableExtra)
library(reshape2)
```

# Wstęp
<!--  można napisać trochę więcej -->

```{r, results='asis', echo=FALSE}
mycat <- function(text){
  cat(gsub(pattern = "\n", replacement = "  \n", x = text))
}
```

Sprawozdanie zawiera rozwiązanie zadań z listy 2. Dotyczą one zagadnień dyskretyzacji i redukcji wymiaru.

# Zadanie 1

W pierwszym zadaniu mamy dokonać dyskretyzacji cech ciągłych ze zbioru `iris` i ocenić jej jakość.

## Wczytanie danych i wstępna analiza
```{r}
data(iris)
```

Wybierzmy zmienne o najlepszej i najgorszej zdolności dyskryminacyjnej. W tym celu narysujemy wykresy pudełkowe oraz wyliczymy współczynniki zmienności każdej ze zmiennych z podziałem na poszczególne gatunki irysów i porównamy ich rozkłady.

```{r include=FALSE}
str(iris)
plot_intro(iris)
```

```{r}
plot_boxplot(iris, by="Species")
```

```{r, echo=FALSE}
table <- aggregate(. ~ Species, iris, cv)
table %>% kbl(caption="Wspolczynniki zmiennosci dla poszczegolnych zmiennych", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position', 'scale_down'))
```

Możemy zauważyć, że zmienna Petal.Length najefektywniej rozdziela poszczególne gatunki, natomiast zmienna Sepal.Width radzi sobie z tym najgorzej.

## Metody dyskretyzacji

Porównamy ze sobą cztery metody dyskretyzacji nienadzorowanej:

 - equal width,
 - equal frequency,
 - k-means clustering,
 - dyskretyzację dla przedziałów zadanych przez użytkownika.

### Najlepiej separująca zmienna

Zacznijmy od zmiennej Petal.Length, która najlepiej rozdziela poszczególne gatunki irysów.


```{r message=FALSE, fig.height=3, fig.width=6, echo=FALSE}
intervals <- c(min(iris$Petal.Length), 2, 5, max(iris$Petal.Length))

for (method in c("interval", "frequency", "cluster", "fixed")) {
  petal.length.discretized <- if (method != "fixed") 
    discretize(iris$Petal.Length, method=method) else 
    discretize(iris$Petal.Length, method=method, breaks=intervals)
  print(plot_grid(
    ggplot(iris, aes(Petal.Length)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(petal.length.discretized)$"discretized:breaks") +
          ggtitle(method),
    ggplot(iris, aes(Species, Petal.Length)) +
      geom_quasirandom(aes(col=Species)) +
      scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
      geom_hline(yintercept=attributes(petal.length.discretized)$"discretized:breaks") +
      theme(legend.position = "none")))
  discretized.table <- table(petal.length.discretized, iris$Species)
  matchClasses(discretized.table)
  # discretized.table <- table(petal.length.discretized, iris$Species)
  # percentage <- round(100 * sum(diag(discretized.table)) / sum(discretized.table), 2)
  # mycat(paste0("\n Poprawnie sklasyfikowane obserwacje: ", percentage, "%. \n"))
}
```

### Najgorzej separująca zmienna

Możemy zobaczyć teraz jak poszczególne metody działają dla zmiennej Sepal.Width, która najgorzej radzi sobie z rozdzielaniem gatunków.

```{r message=FALSE, fig.height=3, fig.width=3.25, echo=FALSE}
intervals <- c(min(iris$Sepal.Width), 2.5, 3, max(iris$Sepal.Width))
for (method in c("interval", "frequency", "cluster", "fixed")) {
  sepal.width.discretized <- if (method != "fixed") 
    discretize(iris$Sepal.Width, method=method) else 
    discretize(iris$Sepal.Width, method=method, breaks=intervals)
  print(ggplot(iris, aes(Sepal.Width)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
          ggtitle(method))
  print(ggplot(iris, aes(Species, Sepal.Width)) +
    geom_quasirandom(aes(col=Species)) +
    scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
    geom_hline(yintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
    theme(legend.position = "none"))
  discretized.table <- table(sepal.width.discretized, iris$Species)
  matchClasses(discretized.table)
}
```

Dla obu zmiennych każda z metod wypada równie dobrze, przy czym, najlepsze wyniki produkują metody równej częstości oraz k-średnich.


## Metody dyskretyzacji z wartościami odstającymi

Rozpatrzmy teraz dyskretyzację przy dodaniu sztucznie wartości odstających.

### Zmienna Petal.Length

Zacznijmy znowu od zmiennej Petal.Length.

```{r message=FALSE, fig.height=3, fig.width=3.25, echo=FALSE}
iris$Petal.Length[which.min(iris$Petal.Length)] <- min(iris$Petal.Length) - IQR(iris$Petal.Length)
iris$Petal.Length[which.max(iris$Petal.Length)] <- max(iris$Petal.Length) + IQR(iris$Petal.Length)
intervals <- c(min(iris$Petal.Length), 2, 5, max(iris$Petal.Length))
for (method in c("interval", "frequency", "cluster", "fixed")) {
  petal.length.discretized <- if (method != "fixed") 
    discretize(iris$Petal.Length, method=method) else 
    discretize(iris$Petal.Length, method=method, breaks=intervals)
  print(ggplot(iris, aes(Petal.Length)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(petal.length.discretized)$"discretized:breaks") +
          ggtitle(method))
  print(ggplot(iris, aes(Species, Petal.Length)) +
    geom_quasirandom(aes(col=Species)) +
    scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
    geom_hline(yintercept=attributes(petal.length.discretized)$"discretized:breaks") +
    theme(legend.position = "none"))
  discretized.table <- table(petal.length.discretized, iris$Species)
  matchClasses(discretized.table)
}
```

### Zmienna Sepal.Width

Dla zmiennej Sepal.Width po dodaniu wartości odstających dyskretyzacja wygląda następująco:

```{r message=FALSE, fig.height=3, fig.width=3.25, echo=FALSE}
iris$Sepal.Width[which.min(iris$Sepal.Width)] <- min(iris$Sepal.Width) - IQR(iris$Sepal.Width)
iris$Sepal.Width[which.max(iris$Sepal.Width)] <- max(iris$Sepal.Width) + IQR(iris$Sepal.Width)
intervals <- c(min(iris$Sepal.Width), 2.5, 3, max(iris$Sepal.Width))
for (method in c("interval", "frequency", "cluster", "fixed")) {
  sepal.width.discretized <- if (method != "fixed") 
    discretize(iris$Sepal.Width, method=method) else 
    discretize(iris$Sepal.Width, method=method, breaks=intervals)
  print(ggplot(iris, aes(Sepal.Width)) +
          geom_histogram() +
          geom_vline(xintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
          ggtitle(method))
  print(ggplot(iris, aes(Species, Sepal.Width)) +
    geom_quasirandom(aes(col=Species)) +
    scale_color_manual(values=wes_palette("GrandBudapest1", 3)) +
    geom_hline(yintercept=attributes(sepal.width.discretized)$"discretized:breaks") +
    theme(legend.position = "none"), caption="test")
  discretized.table <- table(sepal.width.discretized, iris$Species)
  matchClasses(discretized.table)
}
```

Nie powinien dziwić fakt, że największa zmiana w poprawności predykcji dotknęła metodę przedziałów równej długości, gdyż pojedyncza obserwacja całkowicie zmienia dobór miejsc partycji przedziału.

# Zadanie 2

## Wczytanie i przygotowanie danych

Teraz naszym zadaniem jest dokonanie analizy składów głównych (PCA) dla zbioru `state.x77`, który zawiera informacje o wskaźnikach terytorialno-społecznych.

Najpierw wczytajmy dane i uzupełnijmy je o informacje geograficzne.

```{r state dataset}
data(state)
state <- as.data.frame(state.x77)
state$region <- state.region
state$division <- state.division
state.subset <- subset(state, select=-c(region, division))
```

By rozstrzygnąć, czy potrzebna jest normalizacja danych, przeanalizujemy wykresy pudełkowe oraz wyznaczymy odchylenia standardowe i współczynniki zmienności.

```{r data boxplot, message=FALSE, echo=FALSE, fig.height=2.8, fig.cap="Wykresy pudelkowe dla zmiennych ze zbioru state.x77"}
ggplot(data=melt(state.subset), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable))
```

```{r tabela_1, echo=FALSE, eval=TRUE, results='asis'}
var <- sapply(state.subset, function(X) c(sd(X), cv(X)))
summary.matrix <- as.matrix(var)
row.names(summary.matrix) <- c("Odchylenie standardowe", "Wspolczynnik zmiennosci")
summary.matrix %>% kbl(caption="Odchylenie standardowe i wspolczynnik zmiennosci dla zmienych", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position', 'scale_down'))
```

Widać, że zmienne wymagają standaryzacji --- ich wariancje zbyt mocno się różnią.

## Składowe główne i ich analiza

Wyznaczymy teraz składowe główne i przedstawimy ich rozrzut, wykorzystując wykresy pudełkowe.

```{r pca}
after.pca <- prcomp(state.subset, retx=T, center=T, scale.=T)
```

```{r pca boxplot, echo=FALSE, message=FALSE, fig.height=2.9, fig.cap="Wykresy pudelkowe dla skladowych glownych"}
ggplot(data = melt(data.frame(after.pca$x)), 
       aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable))
```

Przypatrzmy się teraz wektorom ładunków dla trzech pierwszych składowych głównych.

```{r loadings table, echo=FALSE}
matrix = as.matrix(after.pca$rotation[, c(1:3)])
matrix %>% kbl(caption="Wektory ladunkow dla trzech pierwszych PC", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position'))
```

- W przypadku pierwszej składowej głównej, największy wkład mają zmienne `Illiteracy`, `Murder`, `HS Grad` i `Life Exp`. Dwie pierwsze mają ten sam znak, możemy więc wnioskować, że są ze sobą powiązane. `HS Grad` i `Life Exp` mają znak przeciwny --- stąd te dwie pary są ze sobą negatywnie skorelowane. Jest to dość oczywisty rodzaj zależności między stopniem analfabetyzmu a procentem ilości mających ukończoną szkołę średnią i między ilością morderstw a średnią długością życia.

- W przypadku drugiej składowej głównej, największą wagę mają zmienne `Area`, `Population` i `Income`. Zależność między `Area` a `Population` jest dość oczywista, natomiast zależność tych zmiennych od `Income` już niekoniecznie da się łatwo wytłumaczyć. 

Zbadajmy teraz jaka część wyjaśnionej wariancji odpowiada kolejnym składowym głównym.


```{r variance, echo=FALSE, fig.width=8, fig.cap="Wariancja wyjasniana przez poszczegolne skladowe glowne i wariancja skumulowana."}
variance_proportion <- (after.pca$sdev ^2) / sum(after.pca$sdev^2)

df <- data.frame(variance_proportion)

df["cumulative.variance"] <- cumsum(variance_proportion)

labels <- paste0("PC", 1:8)
df["PC"] = labels

barplot1 <- ggplot(data=df,
                   aes(x=PC, y=variance_proportion, fill=PC)) + 
                   geom_bar(stat="identity") + scale_fill_brewer(palette="Dark2")
barplot2 <- ggplot(data=df, aes(x=PC, y=cumulative.variance, fill=PC)) + 
                    geom_bar(stat="identity") + scale_fill_brewer(palette="Dark2")
plot_grid(barplot1, barplot2, ncol=2, nrow=1)
```


```{r table, echo=FALSE}
variance.matrix <- t(df[1:5, 1:2])
colnames(variance.matrix) <- paste0("PC", 1:5)
rownames(variance.matrix) <- c("Proporcja wariancji", "Skumulowana wariancja")
variance.matrix %>% kbl(caption="Odchylenie standardowe i wspolczynnik zmiennosci dla zmienych", format="latex", digits=3) %>% kable_styling(latex_options=c('hold_position'))
```


Zauważamy, że:

- PC1 wyjaśnia $45 \%$ wyjaśnianej wariancji, PC2 prawie $25 \%$;
- $80 \%$ całkowitej wariancji jest wyjaśniane przez pierwsze cztery składowe główne (trzy pierwsze wyjaśniają niewiele mniej), $90 \%$ jest wyjaśniane zaś przez pierwszych 5.


## Wizualizacja danych

W tej części wygenerujemy wykresy rozrzutu 2d dla dwóch pierwszych składowych głównych. Skorzystamy z danych dotyczących lokalizacji poszczególnych stanów, by być w stanie wyciągnąc pewne interesujące wnioski.


```{r 2d plot region, echo=FALSE, fig.width=10, fig.height=3.5, fig.cap="Wykresy rozrzutu"}
p1 <- ggplot(data = data.frame(after.pca$x), aes(x=PC1, y=PC2)) + 
        geom_text(aes(color = state$region, label = rownames(state)), size = 2, nudge_y = 0.2) + 
        geom_point(aes(color= state$region))

p2 <- ggplot(data = data.frame(after.pca$x), aes(x=PC1, y=PC2)) + 
        geom_text(aes(color = state$division, label = rownames(state)), size = 2, nudge_y = 0.2) + 
        geom_point(aes(color = state$division))

plot_grid(p1, p2, nrow = 1, ncol = 2)
```


Obserwacje:

- Stany zlokalizaowane w południowych częściach USA są stosunkowo blisko względem siebie położone --- możemy więc wnioskować o ich dużym podobieństwie. Są one też często dość oddalone od pozostałych obserwacji. 


```{r Alaska and California stats, echo=FALSE, results='hide'}
rownames(state)[which.max(state$Population)]
rownames(state)[which.max(state$Income)]
rownames(state)[which.max(state$Area)]
```

- Są dwie obseracje, które znacząco różnią się od pozostałych. Są to Alaska i Kalifornia. Alaska jest stanem o największej powierzchni oraz dochód na jednego mieszkańca jest tam również najwyższy. Natomiast w Kalifornii mieszka najwięcej ludzi (stan ten charakteryzuje się także dużą powierzchnią).


Przygotowaliśmy także wykresy 3d --- kod umieściliśmy w dodatkowym skrypcie. 

## Korelacja zmiennych

Zbadamy teraz korelację między zmiennymi. Najpierw skorzystamy z dwuwykresu.

```{r biplot, echo=FALSE, fig.cap="Dwuwykres dla danych state.x77", fig.width=6, fig.height=3}
ggbiplot(after.pca, obs.scale = 2, var.scale = 2, varname.size = 3.7, ellipse = TRUE, 
         groups = state$region)
```

Możemy zaobserwować, że zmienne `Murder` i `Illiteracy` są ze dodatnią skorelowane. Podobnie zachowują się zmienne `Income` i `HS Grad`. Ujemna korelacja jest możliwa do zaobserwowania pomiędzy zmiennymi `Life Exp` i `Murder`, `HS Grad` i `Illiteracy`. Zmienna `Frost` jest ujemnie skorelowane z `Illiteracy` i `Murder`.

Te wnioski potwierdzają, jeżeli popatrzymy na mapę ciepła korelacji.

```{r, echo=FALSE, fig.cap="Mapa ciepla korelacji zmiennych"}
cor.matrix <- cor(state.subset)
ggcorrplot(cor.matrix, lab = TRUE)
```

## Wnioski do zadania 2

Dzięki zastosowaniu metody analizy składowych głównych udało się nam otrzymać ciekawe wnioski dotyczące stanów USA.

Przede wszystkim, stany zlokalizowane na południu kraju są bardzo do siebie podobne. Charakteryzują się największym stopniem analfabetyzmu, największą ilością morderstw (co potwierdzała ujemna korelacja tych zmiennych ze zmienną `Frost`), najniższym stopniem wykształcenia. 


# Zadanie 3

Wybranym przez nas zbiorem danych jest &hellip;

Wczytajmy dane i przygotujmy je do do skalowania wielowymiarowego.


Porównamy teraz jakoś odwzorowania MDS w zależności od wielkości wymiaru $d$ przestrzeni docelowej. Przedstawimy na wykresie wartości funkcji STRESS, jak i wykonamy diagramy Sheparda.




