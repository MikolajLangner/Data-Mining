---
title: "Raport 4"
subtitle: "Eksploracja danych"
author:   |
          |    Mikołaj Langner, Marcin Kostrzewa
          |    nr albumów: 255716, 255749
date: "2021-05-28"
output: 
  pdf_document:
    toc: true
    fig_caption: yes
    fig_width: 5 
    fig_height: 4 
    number_sections: true
    includes:
      in_header: "preambula.tex" 
fontsize: 12pt 
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4, fig.pos='H')
library(ggplot2)
library(caret)
library(kernlab)
library(rattle)
library(factoextra)
library(ipred)
library(adabag)
library(randomForest)
library(dplyr)
library(emdbook)
library(rpart)
library(xtable)
library(tidyr)
library(ggfortify)
library(cluster)
library(cowplot)
library(clValid)
library(e1071)
library(reshape2)
library(DataExplorer)

bagging = ipred::bagging
```


# Wstęp

Niniejszy raport zawiera rozwiązania rozwiązania zadań z listy 4.

W zadaniu pierwszym zastosujemy zaawansowane metody klasyfikacji:
\begin{itemize}
\item bagging,
\item boosting,
\item random forest,
\item metodę wektorów nośnych (SVM),
\end{itemize}



W zadaniu drugim badamy jakość 


\newpage

# Zadanie 1

```{r load.data, echo=FALSE}
data(wine)
n = nrow(wine)

set.seed(42)
train.index <- sample(n, 2/3 * n)
train.data <- wine %>% slice(train.index)
test.data <- wine %>% slice(-train.index)
train.subset <- data.frame(train.data[, c(1, 2, 8)])
test.subset <- data.frame(test.data[, c(1, 2, 8)])
train.etiquettes <- train.data$Type
test.etiquettes <- test.data$Type

cv <- trainControl(method="cv", number=5)
```

## a)

Naszym zadaniem będzie zbadanie tego jak poprawia się jakość klasyfikacji, jeżeli zamiast pojedynczego drzewa decyzyjnego, użyjemy złożonego klasyfikatora.

### Pojedyncze drzewo decyzyjne

Przypomnijmy najpierw jak radziła sobie metoda drzewa klasyfikacyjnego.

```{r decision_tree}
tree.model <- rpart(Type ˜ ., data = train.subset, cp=0)
```

Wyglądało ono następująco --- rysunek (\ref{fig::fig1}).

```{r basic_tree_plot, echo=FALSE, eval=TRUE, width=4.5, height=3,fig.cap="\\label{fig::fig1}Pojedyncze drzewo decyzyjne."}
fancyRpartPlot(tree.model, sub="")
```

```{r decision_tree_error, echo=FALSE}
predictor  <- function(model, newdata) 
{ predict(model, newdata=newdata, type = "class") }

decision.tree.predictor <- function(formula, data) 
{ rpart(formula, data = data, cp = 0)}

decision.tree.error.cv <- errorest(Type~., wine[, c(1, 2, 8)], 
                                   model=decision.tree.predictor, 
                                   predict=predictor, estimator="cv", 
                                   est.para=control.errorest(k = 5))
decision.tree.error.boot <- errorest(Type~., wine[, c(1, 2, 8)], 
                                     model=decision.tree.predictor, 
                                     predict=predictor, estimator="boot", 
                                     est.para=control.errorest(nboot = 25))

decision.tree.error.632 <- errorest(Type~., wine[, c(1, 2, 8)], 
                                     model=decision.tree.predictor, 
                                     predict=predictor, estimator="632plus", 
                                     est.para=control.errorest(nboot = 25))
```


Błędy predykcji wyznaczone za pomocą metod: $5$-krotnej walidacji krzyżowej, bootstrap oraz \verb|.632+|, wyniosły kolejno `r decision.tree.error.cv$error`, `r decision.tree.error.boot$error` oraz `r decision.tree.error.632$error`.


### Bagging

Najpierw skorzystamy z algorytmu bagging. Znajdziemy optymalną wartość dla parametru \verb|nbagg|.

```{r nbagg}
B.vector <- c(1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
bagging.error.rates <- sapply(B.vector, function(b)  
  {errorest(Type~., data=train.data, model=bagging, 
            nbagg=b, estimator="632plus",
            est.para=control.errorest(nboot = 20))$error})
choice <- B.vector[which.min(bagging.error.rates)]
```

```{r nbagg_plot, eval=TRUE, echo=FALSE, fig.width=5.5, fig.height=3, fig.cap="\\label{fig::2} Wplyw ilosci replikacji na blad klasyfikacji."}
ggplot(data = data.frame(B=B.vector, x=bagging.error.rates), aes(B, x)) + 
  geom_line(color="red") + geom_point() + xlab('Ilosc replikacji') + ylab('Wartosc bledu')
```

Jak widać, najlepiej zbudować model dla nbagg równego `r choice`. Parametr złożoności \verb|cp| przyjmujemy równy $0$, tak jak w przypadku pojedynczego drzewa.

```{r bagging_pred}
bagging.start <- Sys.time()
bagging.model <- bagging(Type~., data=train.data, nbagg=choice,
                         minsplit=1, cp=0)
bagging.end <- Sys.time()
bagging.train.pred <- predict(bagging.model, train.data)
bagging.test.pred <- predict(bagging.model, test.data)
```

Wyznaczymy dla tego modelu macierze pomyłek i wartości błędów klasyfkacji.

```{r bagging_confusion, echo=FALSE, eval=TRUE}
test.confusion <- table(bagging.test.pred, test.etiquettes)
train.confusion <- table(bagging.train.pred, train.etiquettes)

print(xtable(train.confusion), file="bag1.tex", floating=FALSE)
print(xtable(test.confusion), file="bag2.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:bag1}{\input{./bag1}}}\quad
\subfloat[Zbior testowy]{\label{tab:bag2}{\input{./bag2}}}
\caption{Macierze pomylek dla algorytmu bagging.}
\label{tab:tab_bag}
\end{table}

Błędy klasyfikacji to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.

Wyznaczymy teraz dla tego modelu klasyfikacyjnego błędy predykcji podobnie jak dla drzewa decyzyjnego.

```{r bagging_errors}
predictor  <- function(model, newdata) 
{predict(model, newdata=newdata, type = "class")}

bagging.predictor <- function(formula, data) 
{bagging(formula, data = data, nbagg = choice, cp = 0)}

bagging.error.cv <- errorest(Type~., wine, 
                                   model=bagging.predictor, 
                                   predict=predictor, estimator="cv", 
                                   est.para=control.errorest(k = 5))
bagging.error.boot <- errorest(Type~., wine, 
                                     model=bagging.predictor, 
                                     predict=predictor, estimator="boot", 
                                     est.para=control.errorest(nboot = 25))
bagging.error.632 <- errorest(Type~., wine, 
                                     model=bagging.predictor, 
                                     predict=predictor, estimator="632plus", 
                                     est.para=control.errorest(nboot = 25))
```

Błędy wyniosły kolejno `r bagging.error.cv$error`, `r bagging.error.boot$error` oraz `r bagging.error.632$error`.


### Boosting

Wykorzystamy teraz algorytm boosting --- skorzystamy z funkcji \verb|boosting| z pakietu \verb|adabag|.

Najpierw dobierzemy optymalnie wartość parametru \verb|mfinal| --- ilość wykorzystanych przez algorytm drzew.

```{r mfinal}
mfinal.choice <- 20
mfinal.vector <- seq(5, 100, by=20)
predictor  <- function(model, newdata)
{ as.factor(predict(model, newdata=newdata, type = "class")$class)}
boosting.error.rates <- sapply(mfinal.vector, function(m)
  { errorest(Type~., wine, predict=predictor,
           model=boosting, mfinal=m,
           estimator="cv",
           est.para=control.errorest(k = 5))$error})
mfinal.choice <- mfinal.vector[which.min(boosting.error.rates)]
```

```{r mfinal_plot, echo=FALSE, eval=TRUE, fig.cap="\\label{fig::fig3} Zaleznosc bledu od ilosci drzew."}
ggplot(data = data.frame(M=mfinal.vector, error=boosting.error.rates), aes(M, error)) +
  geom_line(color="red") + geom_point() + xlab('Ilosc drzew') + ylab('Wartosc bledu')
```

Wybieramy wartość mfinal równą `r mfinal.choice`.

```{r boosting_model}
boosting.start <- Sys.time()
boosting.model <- boosting(Type~., data = train.data, boos = TRUE, 
                           mfinal = mfinal.choice)
boosting.end <- Sys.time()
boosting.test.pred <- predict(boosting.model, test.data)
boosting.train.pred <- predict(boosting.model, train.data)
```

```{r boosting_matrices}
test.confusion <- boosting.test.pred$confusion
train.confusion <- boosting.train.pred$confusion

print(xtable(train.confusion), file="boost1.tex", floating=FALSE)
print(xtable(test.confusion), file="boost2.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:boost1}{\input{./boost1}}}\quad
\subfloat[Zbior testowy]{\label{tab:boost2}{\input{./boost2}}}
\caption{Macierze pomylek dla algorytmu boosting.}
\label{tab:tab_boost}
\end{table}

Błędy klasyfikacji to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.


Wyznaczymy też teraz błędy predykcji (tym razem z racji złożoności obliczeniowej i długiego czasu wykonania tylko dla metody $5$-krotnej walidacji krzyżowej).


```{r boosting_error}
boosting.predictor <- function(formula, data) 
{ boosting(formula, data = data, mfinal=mfinal.choice, boos = TRUE)}

boosting.error.cv <- errorest(Type~., wine, 
                                   model=boosting.predictor, 
                                   predict=predictor, estimator="cv", 
                                   est.para=control.errorest(k = 5))
```

Błąd wyniosł `r boosting.error.cv$error`.


### Random Forest

Teraz wykorzystamy algorytm random forest.

Postaramy się odpowiednio dobrać parametry \verb|ntree| (ilość drzew) i \verb|mtry| (ilość losowo wybieranych cech).


```{r ntree_mtry}
ntree.vector <- seq(10, 500, by=20)
ntree.error.rates <- sapply(ntree.vector, function(b)  
  {errorest(Type~., data=train.data, model=randomForest, 
            ntree=b, estimator="632plus",
            est.para=control.errorest(nboot = 20))$error})
ntree.choice <- ntree.vector[which.min(ntree.error.rates)]

mtry.vector <- seq(1, sqrt(ncol(wine))+1, by=1)
mtry.error.rates <- sapply(mtry.vector, function(m) 
  {errorest(Type~., data=train.data, model=randomForest, ntree = ntree.choice, 
            mtry=m, estimator="632plus",
            est.para=control.errorest(nboot = 20))$error})
mtry.choice <- mtry.vector[which.min(mtry.error.rates)]
```

```{r mtry_ntree_plots, echo=FALSE, eval=TRUE, fig.width=6.5, fig.height=3.5, fig.cap="\\label{fig::fig4} Wykresy zaleznosci bledu klasyfikacji od parametrow mtry i ntree."}
p1 <- ggplot(data = data.frame(ntree=ntree.vector, error=ntree.error.rates), aes(ntree, error)) + 
  geom_line(color="red") + geom_point() 

p2 <- ggplot(data = data.frame(mtry=mtry.vector, error=mtry.error.rates), aes(mtry, error)) + 
  geom_line(color="red") + geom_point() 

plot_grid(p1, p2)
```

Podobnie jak wcześniej wyznaczamy za pomocą modelu etykietki klas i wyznaczamy macierze pomyłek i błędy klasyfikacji.

```{r randomforrest_model}
rf.start <- Sys.time()
rf.model <- randomForest(Type~., data = train.data, ntree=ntree.choice,
                         mtry=mtry.choice, importance=TRUE)
rf.end <- Sys.time()
rf.test.pred <- predict(rf.model, test.data)
rf.train.pred <- predict(rf.model, train.data)
```

```{r confusion_tables_rf, echo=FALSE, eval=TRUE}
test.confusion <- table(rf.test.pred, test.etiquettes)
train.confusion <- table(rf.train.pred, train.etiquettes)

print(xtable(train.confusion), file="rf1.tex", floating=FALSE)
print(xtable(test.confusion), file="rf2.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:rf1}{\input{./rf1}}}\quad
\subfloat[Zbior testowy]{\label{tab:rf2}{\input{./rf2}}}
\caption{Macierze pomylek dla algorytmu randomForest.}
\label{tab:tab_rf}
\end{table}

Błędy klasyfikacji to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.

Tak jak dla wcześniejszych algorytmów, wyznaczymy teraz błędy predykcji.

```{r rf_errors}
predictor  <- function(model, newdata) 
{ predict(model, newdata=newdata, type = "class") }

rf.predictor <- function(formula, data) 
{ randomForest(formula, data = data, ntree = ntree.choice, mtry = mtry.choice)}

rf.error.cv <- errorest(Type~., wine, model=rf.predictor, 
                                   predict=predictor, estimator="cv", 
                                   est.para=control.errorest(k = 5))
rf.error.boot <- errorest(Type~., wine, model=rf.predictor, 
                                     predict=predictor, estimator="boot", 
                                     est.para=control.errorest(nboot = 25))
rf.error.632 <- errorest(Type~., wine, model=rf.predictor, 
                                     predict=predictor, estimator="632plus", 
                                     est.para=control.errorest(nboot = 25))
```

Wyniosły one kolejno `r rf.error.cv$error`, `r rf.error.boot$error` oraz `r rf.error.632$error`.


Wykorzystamy teraz algorytm random forest do wyznaczenia rankingu cech (\textit{variable importance}). 

```{r importance_plot, echo=FALSE, eval=TRUE, fig.cap="\\label{fig::fig5} Wykres waznosci zmiennych."}
acc = data.frame(variable=as.vector(colnames(wine))[-1], val=importance(rf.model)[, 4]) %>%
  arrange(desc(val))
gini = data.frame(variable=as.vector(colnames(wine))[-1], val=importance(rf.model)[, 5]) %>%
  arrange(desc(val))

p1 <- ggplot(data = acc, aes(x=reorder(variable, val), y=val, fill=variable)) + geom_bar(stat="identity") + coord_flip() + theme(legend.position="none") + xlab("Zmienne") + ylab("MeanAccuracyDecrease")
p2 <- ggplot(data = gini, aes(x=reorder(variable, val), y=val, fill=variable)) + geom_bar(stat="identity") + coord_flip() + theme(legend.position="none") + xlab("") + ylab("MeanGiniDecrease")

plot_grid(p1, p2)
```

Przypomnijmy, że na ostatniej liście za zmienne istotne, takie, które dobrze dywersyfikowały klasy ze zbioru \verb|wine|, były \verb|Alcohol| i \verb|Flavanoids|. Tak jak widzimy to na rysunku (\ref{fig::fig5}) dokonaliśmy wtedy całkiem dobrej decyzji, ponieważ zmienne te są w czołówce najważniejszych zmiennych. Wykresy wskazują, że najważniejsze są zmienne \verb|Color| i \verb|Proline|, które w trakcie dokonywania wyboru, odrzuciliśmy. 

### Wnioski

Podsumujmy uzyskane rezultaty w tabeli (\ref{tab:tab_last}).

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\hline
Metoda & Drzewa decyzyjne & bagging & boosting & random forest \\
\hline
CV & `r decision.tree.error.cv$error` & `r bagging.error.cv$error` & `r boosting.error.cv$error` & `r rf.error.cv$error` \\
Bootstrap & `r decision.tree.error.boot$error` & `r bagging.error.boot$error` & NaN & `r rf.error.boot$error`\\
632+ & `r decision.tree.error.632$error` &  `r bagging.error.632$error`& NaN & `r rf.error.632$error`\\
\hline
\end{tabular}
}
\caption{Wartości błędów predykcji.}
\label{tab:tab_last}
\end{table}

Jak widzimy każda rozpatrywana metoda jest znacząco lepsza od pojedynczego drzewa klasyfikacyjnego. Najlepiej z nich poradził sobie algorytm random forest.

Mieliśmy także porównać czasy potrzebne do zbudowania modelu dla kolejnych metod. Wynosiły one:

\begin{itemize}
  \item dla bagging `r bagging.end - bagging.start`,
  \item dla boosting `r boosting.end - boosting.start`,
  \item dla random forest `r rf.end - rf.start`.
\end{itemize}

Widać, że algorytm boosting potrzebuje znacznie więcej czasu na stworzenie modelu od pozostałych algorytmów.
## b)

```{r}
wine <- wine %>% select(c(Type, Alcohol, Flavanoids))
```

```{r SVM.class.err, echo=FALSE, fig.cap="Dokładność klasyfikatora od parametru kosztu"}
models <- train(Type~.,
               data = wine,
               method = "svmLinear",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5)))
ggplot(models$results, aes(C, Accuracy)) + geom_line() + scale_y_log10() + scale_x_log10()
```

```{r SVM.decision.bound.01, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=0.1$"}
model <- ksvm(Type~., data = wine, kernel = "vanilladot", C = 1e-1)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.1, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=1$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e0)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.10, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=10$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e1)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.100, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=100$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e2)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.1000, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=1000$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e3)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```


```{r SVM.kernels, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
models.linear <- train(Type ~.,
               data = wine,
               method = "svmLinear",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5)))
models.poly <- train(Type ~.,
               data = wine,
               method = "svmPoly",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5), degree = seq(1, 5, 1), scale = seq(.2, 1, .2)))
models.radial <- train(Type ~.,
               data = wine,
               method = "svmRadial",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5), sigma = lseq(1e-4, 1e0, 5)))

result.linear <- models.linear$results[which.max(models.linear$results$Accuracy), "Accuracy"]
result.poly <- models.poly$results[which.max(models.poly$results$Accuracy), "Accuracy"]
result.radial <- models.radial$results[which.max(models.radial$results$Accuracy), "Accuracy"]
print(xtable(as.tibble(list(linear=result.linear, polynomial=result.poly, radial=result.radial)), digits=3, caption="Porównanie klasyfikatorów dla różnych jąder"), include.rownames=FALSE, comment=FALSE)
```

```{r tune.radial, echo=FALSE, message=FALSE, fig.cap="Mapa ciepła dokładności klasyfikatora"}
models.radial <- train(Type ~.,
               data = wine,
               method = "svmRadial",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5), sigma = lseq(1e-4, 1e0, 5)))
ggplot(models.radial$results, aes(C, sigma, fill=Accuracy)) + geom_tile() + scale_y_log10() + scale_x_log10()
```

```{r tune.radial.table, results='asis', echo=FALSE, message=FALSE}
print(xtable(models.radial$bestTune, caption = "Parametry dla najlepszego klasyfikatora"), include.rownames=FALSE, comment=FALSE)
```

# Zadanie 2

W tym zadaniu zastosujemy algorytmy analizy skupień do wyznaczenia klastrów dla zbioru \verb|wine|,
ocenimy ich skuteczność i porównamy je ze sobą. Sięgniemy po dwa algorytmy: PAM i AGNES.

## Wizualizacja wyników grupowania ($K=3$)

Najpierw wyznaczymy macierz niepodobieństwa dla naszych danych. 

```{r wine subset}
data(wine)
wine.subset = wine[, -1]
diss.matrix <- daisy(wine.subset, stand=TRUE) %>% as.matrix
group.colors <- as.numeric(wine$Type)
```

Przyjrzyjmy się najpierw jakie wyniki daje nam zastosowanie algorytmu PAM.

```{r clusters pam, echo=FALSE, message=FALSE, warning=FALSE, fig.height=7, fig.cap="Skupienia dla metody PAM"}
data(wine)
clusters <- pam(scale(wine %>% select(-Type)), 3)
wine$Predicted <- as.factor(clusters$clustering)
p1 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Predicted', shape='Type', frame=TRUE, frame.type='norm')
p2 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Type', shape='Predicted', frame=TRUE, frame.type='norm')
plot_grid(p1, p2, ncol=1)
```


Zobaczmy teraz, jak poradził sobie algorytm AGNES z single-linkage.


```{r clusters agnes single, echo=FALSE, message=FALSE, fig.height=7, warning=FALSE, fig.cap="Skupienia dla metody AGNES z single-linkage"}
data(wine)
agnes.single <- agnes(diss.matrix,  method='single', diss = T)
clusters <- cutree(agnes.single, 3)
wine$Predicted <- as.factor(clusters)
p1 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Predicted', shape='Type', frame=TRUE, frame.type='norm')
p2 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Type', shape='Predicted', frame=TRUE, frame.type='norm')
plot_grid(p1, p2, ncol=1)
```

Zobaczmy jak wygląda dendrogram dla tego modelu.

```{r single_dend, warning=FALSE, echo=FALSE, fig.cap="Dendrogram dla single-linkage."}
agnes.single.colors <- group.colors[agnes.single$order]
fviz_dend(agnes.single, k=3, cex=0.5, label_cols = agnes.single.colors, k_colors=c("grey","orange","blue"),main="Partycja na 3 skupienia a rzeczywiste klasy - single-linkage", rect = T, lower_rect=-0.5, rect_fill=TRUE, rect_border=c("grey","orange","blue"), as.ggplot = TRUE)
```
Poniżej wyniki dla algorytmu AGNES z complete-linkage.

```{r clusters agnes complete, echo=FALSE, message=FALSE, fig.height=7, warning=FALSE, fig.cap="Skupienia dla metody AGNES z complete-linkage"}
data(wine)
agnes.complete <- agnes(diss.matrix, method="complete", diss = T)
clusters <- cutree(agnes.complete, 3)
wine$Predicted <- as.factor(clusters)
p1 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Predicted', shape='Type', frame=TRUE, frame.type='norm')
p2 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Type', shape='Predicted', frame=TRUE, frame.type='norm')
plot_grid(p1, p2, ncol=1)
```

Zobaczmy jak wygląda dendrogram w tym przypadku.

```{r complete_dend, warning=FALSE, echo=FALSE, fig.cap="Dendrogram dla complete-linkage."}
agnes.complete.colors <- group.colors[agnes.complete$order]
fviz_dend(agnes.complete, k=3, cex=0.5, label_cols = agnes.single.colors, k_colors=c("grey","orange","blue"), main="Partycja na 3 skupienia a rzeczywiste klasy - complete-linkage", rect = T, lower_rect=-0.5, rect_fill=TRUE, rect_border=c("grey","orange","blue"), as.ggplot = TRUE)
```

## Ocena jakości grupowania 

W tej części zadania, porównamy ze sobą algorytmy, jakość uzyskanego dzięk nim grupowania w zależności od przyjętej ilości skupień. Wykorzystamy wskaźniki wewnętrzne, jak i zewnętrzne.


### Wkaźniki wewnętrzne

```{r internal, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.cap="Wskaźniki wewnętrzne dla PAM i AGNES z complete-linkage"}
data(wine)
cl.methods <- c("agnes", "pam")
cl.range <- 2:10
internal <- clValid(wine %>% select(-Type),
                    nClust=cl.range, clMethods=cl.methods, validation="internal",
                    method='complete')
summary(internal)
optimalScores(internal)

my_plot <- list()
for(i in internal@measNames){
  p <- as.data.frame(internal@measures[i, ,]) %>%
    tibble::rownames_to_column(var = "cluster") %>%
    mutate(cluster = factor(cluster, levels = as.numeric(cluster))) %>%
    melt() %>%
    ggplot(., aes(x = cluster, y = value, colour = variable, group = variable)) +
    geom_line() +
    geom_point() +
    theme_bw() +
    ggtitle(i)
  my_plot[[i]] <- p
}
 
plot_grid(plotlist = my_plot, ncol = 1)
```


```{r diameter_size_seperation, warning=FALSE, message=FALSE, echo=FALSE}
seperations <- matrix(0, 10, 9)
sizes <- matrix(0, 10, 9)
diameters <- matrix(0, 10, 9)
for (i in 1:9) {
  data <- pam(diss.matrix, k=i+1, diss = T)$clusinfo[, c(1, 4, 5)]
  seperations[, i] <- c(rep(0, 10-i-1), data[, 3])
  sizes[, i] <- c(rep(0, 10-i-1), data[, 1])
  diameters[, i] <- c(rep(0, 10-i-1), data[, 2])
}
mat.colnames <- paste0("K = ", 2:10)
colnames(seperations) <- mat.colnames
colnames(diameters) <- mat.colnames
colnames(sizes) <- mat.colnames

print(xtable(seperations, caption="Seperacja w skupiskach") , file="seperations.tex")
print(xtable(diameters, caption="Srednice skupisk"), file="diameters.tex")
print(xtable(sizes, caption="Rozmiary skupisk"), file="sizes.tex")
```


\input{./seperations}
\input{./diameters}
\input{./sizes}



### Wskaźniki zewnętrzne


```{r external, echo=FALSE, results='hide', fig.cap="Porównanie wskaźników zewnętrznych"}
pam.match <- Vectorize(function(k) {
  pam.table <-  table(pam(diss.matrix, diss = T, k)$clustering, wine$Type)
  sum(diag(pam.table)) / sum(pam.table)
}
)

agnes.single.match <- Vectorize(function(k) {
  agnes.single.table <-  table(cutree(agnes(diss.matrix, method='single', diss = T), k), wine$Type)
  sum(diag(agnes.single.table)) / sum(agnes.single.table)
}
)

agnes.complete.match <- Vectorize(function(k) {
  agnes.complete.table <-  table(cutree(agnes(diss.matrix, method='complete', diss = T), k), wine$Type)
  sum(diag(agnes.complete.table)) / sum(agnes.complete.table)
}
)

df <- data.frame(k = seq(2, 10))
df$pam <- pam.match(df$k)
df$agnes.single <- agnes.single.match(df$k)
df$agnes.complete <- agnes.complete.match(df$k)

ggplot(melt(df, id.vars = 'k', variable.name = 'method', value.name = 'agreement')) +
  geom_line(aes(x=k, y=agreement, color=method))
```


```{r external pam-agnes, echo=FALSE}
matchClasses(table(pam(scale(wine %>% select(-Type)), 3)$clustering,
                   cutree(agnes(scale(wine %>% select(-Type)), method='complete'), 3)))
```


### Ocena otrzymanych rezultatów
```{r cluster boxplots, echo=FALSE}
wine.pam <- pam(scale(wine %>% select(-Type)), 3)
wine.agnes <- cutree(agnes(scale(wine %>% select(-Type)), method='complete'), 3)

wine$pam <- as.factor(wine.pam$clustering)
wine$agnes <- as.factor(wine.agnes)

plot_boxplot(wine, by='pam')
plot_boxplot(wine, by='agnes')
```

```{r medoids, echo=FALSE, results='asis'}
print(xtable(t(data.frame(wine.pam$medoids)), caption="Medoidy dla metody PAM przy K=3"), comment=FALSE)
```

