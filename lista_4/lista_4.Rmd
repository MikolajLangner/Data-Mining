---
title: "Raport 4"
subtitle: "Eksploracja danych"
author:   |
          |    Mikołaj Langner, Marcin Kostrzewa
          |    nr albumów: 255716, 255749
date: "2021-05-28"
output: 
  pdf_document:
    toc: true
    fig_caption: yes
    fig_width: 5 
    fig_height: 4 
    number_sections: true
    includes:
      in_header: "preambula.tex" 
fontsize: 12pt 
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4, fig.pos='H')
library(ggplot2)
library(caret)
library(kernlab)
library(rattle)
library(factoextra)
library(ipred)
library(adabag)
library(randomForest)
library(dplyr)
library(emdbook)
library(rpart)
library(xtable)
library(tidyr)
library(ggfortify)
library(cluster)
library(cowplot)
library(clValid)
library(e1071)
library(reshape2)
library(DataExplorer)

bagging = ipred::bagging
```


# Wstęp

Niniejszy raport zawiera rozwiązania rozwiązania zadań z listy 4.

W zadaniu pierwszym zastosujemy zaawansowane metody klasyfikacji:
\begin{itemize}
\item bagging,
\item boosting,
\item random forest,
\item metodę wektorów nośnych (SVM),
\end{itemize}

W zadaniu drugim badamy jakość 


\newpage

### Wnioski

uwagii, tabelka, wniosek, że klasyfikatory wzmocnione radzą sobie lepiej

## b)

```{r}
wine <- wine %>% select(c(Type, Alcohol, Flavanoids))
```

```{r SVM.class.err, echo=FALSE, fig.cap="Dokładność klasyfikatora od parametru kosztu"}
models <- train(Type~.,
               data = wine,
               method = "svmLinear",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5)))
ggplot(models$results, aes(C, Accuracy)) + geom_line() + scale_y_log10() + scale_x_log10()
```

```{r SVM.decision.bound.01, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=0.1$"}
model <- ksvm(Type~., data = wine, kernel = "vanilladot", C = 1e-1)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.1, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=1$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e0)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.10, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=10$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e1)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.100, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=100$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e2)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

```{r SVM.decision.bound.1000, echo=FALSE, message=FALSE, fig.cap="Obszary decyzyjne dla $C=1000$"}
model <- ksvm(Type ~., data = wine, kernel = "vanilladot", C = 1e3)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids
x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.05), 
                      Flavanoids = seq(min(var2), max(var2), by = 0.05))

df1 <- data.frame(x.test, class = predict(model, x.test))
df2 <- data.frame(x = wine$Alcohol, y = wine$Flavanoids,
                  class = wine$Type)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = .5) + 
  geom_point(aes(x=x, y=y, col=class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```


```{r SVM.kernels, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
models.linear <- train(Type ~.,
               data = wine,
               method = "svmLinear",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5)))
models.poly <- train(Type ~.,
               data = wine,
               method = "svmPoly",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5), degree = seq(1, 5, 1), scale = seq(.2, 1, .2)))
models.radial <- train(Type ~.,
               data = wine,
               method = "svmRadial",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5), sigma = lseq(1e-4, 1e0, 5)))

result.linear <- models.linear$results[which.max(models.linear$results$Accuracy), "Accuracy"]
result.poly <- models.poly$results[which.max(models.poly$results$Accuracy), "Accuracy"]
result.radial <- models.radial$results[which.max(models.radial$results$Accuracy), "Accuracy"]
print(xtable(as.tibble(list(linear=result.linear, polynomial=result.poly, radial=result.radial)), digits=3, caption="Porównanie klasyfikatorów dla różnych jąder"), include.rownames=FALSE, comment=FALSE)
```

```{r tune.radial, echo=FALSE, message=FALSE, fig.cap="Mapa ciepła dokładności klasyfikatora"}
models.radial <- train(Type ~.,
               data = wine,
               method = "svmRadial",
               trControl = trainControl(method = "cv"),
               tuneGrid = expand.grid(C = lseq(1e-1, 1e3, 5), sigma = lseq(1e-4, 1e0, 5)))
ggplot(models.radial$results, aes(C, sigma, fill=Accuracy)) + geom_tile() + scale_y_log10() + scale_x_log10()
```

```{r tune.radial.table, results='asis', echo=FALSE, message=FALSE}
print(xtable(models.radial$bestTune, caption = "Parametry dla najlepszego klasyfikatora"), include.rownames=FALSE, comment=FALSE)
```

# Zadanie 2

W tym zadaniu zastosujemy algorytmy analizy skupień do wyznaczenia klastrów dla zbioru \verb|wine|,
ocenimy ich skuteczność i porównamy je ze sobą. Sięgniemy po dwa algorytmy: PAM i AGNES.

## Wizualizacja wyników grupowania ($K=3$)

Najpierw wyznaczymy macierz niepodobieństwa dla naszych danych. 

```{r wine subset}
data(wine)
wine.subset = wine[, -1]
diss.matrix <- daisy(wine.subset, stand=TRUE) %>% as.matrix
group.colors <- as.numeric(wine$Type)
```

Przyjrzyjmy się najpierw jakie wyniki daje nam zastosowanie algorytmu PAM.

```{r clusters pam, echo=FALSE, message=FALSE, warning=FALSE, fig.height=7, fig.cap="Skupienia dla metody PAM"}
data(wine)
clusters <- pam(scale(wine %>% select(-Type)), 3)
wine$Predicted <- as.factor(clusters$clustering)
p1 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Predicted', shape='Type', frame=TRUE, frame.type='norm')
p2 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Type', shape='Predicted', frame=TRUE, frame.type='norm')
plot_grid(p1, p2, ncol=1)
```


Zobaczmy teraz, jak poradził sobie algorytm AGNES z single-linkage.


```{r clusters agnes single, echo=FALSE, message=FALSE, fig.height=7, warning=FALSE, fig.cap="Skupienia dla metody AGNES z single-linkage"}
data(wine)
agnes.single <- agnes(diss.matrix,  method='single', diss = T)
clusters <- cutree(agnes.single, 3)
wine$Predicted <- as.factor(clusters)
p1 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Predicted', shape='Type', frame=TRUE, frame.type='norm')
p2 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Type', shape='Predicted', frame=TRUE, frame.type='norm')
plot_grid(p1, p2, ncol=1)
```

Zobaczmy jak wygląda dendrogram dla tego modelu.

```{r single_dend, warning=FALSE, echo=FALSE, fig.cap="Dendrogram dla single-linkage."}
agnes.single.colors <- group.colors[agnes.single$order]
fviz_dend(agnes.single, k=3, cex=0.5, label_cols = agnes.single.colors, k_colors=c("grey","orange","blue"),main="Partycja na 3 skupienia a rzeczywiste klasy - single-linkage", rect = T, lower_rect=-0.5, rect_fill=TRUE, rect_border=c("grey","orange","blue"), as.ggplot = TRUE)
```
Poniżej wyniki dla algorytmu AGNES z complete-linkage.

```{r clusters agnes complete, echo=FALSE, message=FALSE, fig.height=7, warning=FALSE, fig.cap="Skupienia dla metody AGNES z complete-linkage"}
data(wine)
agnes.complete <- agnes(diss.matrix, method="complete", diss = T)
clusters <- cutree(agnes.complete, 3)
wine$Predicted <- as.factor(clusters)
p1 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Predicted', shape='Type', frame=TRUE, frame.type='norm')
p2 <- autoplot(prcomp(wine %>% select(-c(Type, Predicted)), scale = TRUE),
         data = wine, colour = 'Type', shape='Predicted', frame=TRUE, frame.type='norm')
plot_grid(p1, p2, ncol=1)
```

Zobaczmy jak wygląda dendrogram w tym przypadku.

```{r complete_dend, warning=FALSE, echo=FALSE, fig.cap="Dendrogram dla complete-linkage."}
agnes.complete.colors <- group.colors[agnes.complete$order]
fviz_dend(agnes.complete, k=3, cex=0.5, label_cols = agnes.single.colors, k_colors=c("grey","orange","blue"), main="Partycja na 3 skupienia a rzeczywiste klasy - complete-linkage", rect = T, lower_rect=-0.5, rect_fill=TRUE, rect_border=c("grey","orange","blue"), as.ggplot = TRUE)
```

## Ocena jakości grupowania 

W tej części zadania, porównamy ze sobą algorytmy, jakość uzyskanego dzięk nim grupowania w zależności od przyjętej ilości skupień. Wykorzystamy wskaźniki wewnętrzne, jak i zewnętrzne.


### Wkaźniki wewnętrzne

```{r internal, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.cap="Wskaźniki wewnętrzne dla PAM i AGNES z complete-linkage"}
data(wine)
cl.methods <- c("agnes", "pam")
cl.range <- 2:10
internal <- clValid(wine %>% select(-Type),
                    nClust=cl.range, clMethods=cl.methods, validation="internal",
                    method='complete')
summary(internal)
optimalScores(internal)

my_plot <- list()
for(i in internal@measNames){
  p <- as.data.frame(internal@measures[i, ,]) %>%
    tibble::rownames_to_column(var = "cluster") %>%
    mutate(cluster = factor(cluster, levels = as.numeric(cluster))) %>%
    melt() %>%
    ggplot(., aes(x = cluster, y = value, colour = variable, group = variable)) +
    geom_line() +
    geom_point() +
    theme_bw() +
    ggtitle(i)
  my_plot[[i]] <- p
}
 
plot_grid(plotlist = my_plot, ncol = 1)
```


```{r diameter_size_seperation, warning=FALSE, message=FALSE, echo=FALSE}
seperations <- matrix(0, 10, 9)
sizes <- matrix(0, 10, 9)
diameters <- matrix(0, 10, 9)
for (i in 1:9) {
  data <- pam(diss.matrix, k=i+1, diss = T)$clusinfo[, c(1, 4, 5)]
  seperations[, i] <- c(rep(0, 10-i-1), data[, 3])
  sizes[, i] <- c(rep(0, 10-i-1), data[, 1])
  diameters[, i] <- c(rep(0, 10-i-1), data[, 2])
}
mat.colnames <- paste0("K = ", 2:10)
colnames(seperations) <- mat.colnames
colnames(diameters) <- mat.colnames
colnames(sizes) <- mat.colnames

print(xtable(seperations, caption="Seperacja w skupiskach") , file="seperations.tex")
print(xtable(diameters, caption="Srednice skupisk"), file="diameters.tex")
print(xtable(sizes, caption="Rozmiary skupisk"), file="sizes.tex")
```


\input{./seperations}
\input{./diameters}
\input{./sizes}



### Wskaźniki zewnętrzne


```{r external, echo=FALSE, results='hide', fig.cap="Porównanie wskaźników zewnętrznych"}
pam.match <- Vectorize(function(k) {
  pam.table <-  table(pam(diss.matrix, diss = T, k)$clustering, wine$Type)
  sum(diag(pam.table)) / sum(pam.table)
}
)

agnes.single.match <- Vectorize(function(k) {
  agnes.single.table <-  table(cutree(agnes(diss.matrix, method='single', diss = T), k), wine$Type)
  sum(diag(agnes.single.table)) / sum(agnes.single.table)
}
)

agnes.complete.match <- Vectorize(function(k) {
  agnes.complete.table <-  table(cutree(agnes(diss.matrix, method='complete', diss = T), k), wine$Type)
  sum(diag(agnes.complete.table)) / sum(agnes.complete.table)
}
)

df <- data.frame(k = seq(2, 10))
df$pam <- pam.match(df$k)
df$agnes.single <- agnes.single.match(df$k)
df$agnes.complete <- agnes.complete.match(df$k)

ggplot(melt(df, id.vars = 'k', variable.name = 'method', value.name = 'agreement')) +
  geom_line(aes(x=k, y=agreement, color=method))
```


```{r external pam-agnes, echo=FALSE}
matchClasses(table(pam(scale(wine %>% select(-Type)), 3)$clustering,
                   cutree(agnes(scale(wine %>% select(-Type)), method='complete'), 3)))
```


### Ocena otrzymanych rezultatów
```{r cluster boxplots, echo=FALSE}
wine.pam <- pam(scale(wine %>% select(-Type)), 3)
wine.agnes <- cutree(agnes(scale(wine %>% select(-Type)), method='complete'), 3)

wine$pam <- as.factor(wine.pam$clustering)
wine$agnes <- as.factor(wine.agnes)

plot_boxplot(wine, by='pam')
plot_boxplot(wine, by='agnes')
```

```{r medoids, echo=FALSE, results='asis'}
print(xtable(t(data.frame(wine.pam$medoids)), caption="Medoidy dla metody PAM przy K=3"), comment=FALSE)
```

