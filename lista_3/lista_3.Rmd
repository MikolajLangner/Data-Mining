---
title: "Raport 3"
subtitle: "Eksploracja danych"
author:   |
          |    Mikołaj Langner, Marcin Kostrzewa
          |    nr albumów: 255716, 255749
date: "2021-05-28"
output: 
  pdf_document:
    toc: true
    fig_caption: yes
    fig_width: 5 
    fig_height: 4 
    number_sections: true
    includes:
      in_header: "preambula.tex" 
fontsize: 12pt 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4, fig.pos='H')
library(DataExplorer)
library(datasets)
library(dplyr)
library(tidyr)
library(xtable)
library(ggplot2)
library(reshape2)
library(wesanderson)
library(e1071)
library(caret)
library(mlbench)
library(class)
library(rpart)
library(rpart.plot)
library(ipred)
library(rattle)
library(naivebayes)
library(nnet)
```


# Wstęp

Raport zawiera rozwiązania listy $3$.

W zadaniu pierwszym zbudujemy klasyfikator na bazie metody regresji liniowej i oceniamy jego skuteczność i dokładność.

W zadaniu drugim porównamy ze sobą rezultaty zastosowania następujących metod klasyfikacji:

\begin{itemize}
  \item metoda k-najblizszych sasiadów (\textit{k-Nearest Neighbors}),
  \item drzewa klasyfikacyjne (\textit{classification trees}),
  \item naiwny klasyfikator bayesowski (\textit{na{\"\i}ve Bayes classifier}),
  \item wieloklasowa regresja logistyczna (\textit{multinomial logistic regression}).
\end{itemize}

# Zadanie 1

## Wczytanie danych i podział na zbiór uczący i testowy

<!--Tutaj jeszcze trochę napiszę o klasach irysów-->
Wczytajmy dane o irysach i podzielmy je na zbiór uczący i testowy w proporcji $1:2$.

```{r}
data(iris)
n <- dim(iris)[1]

train.set.index <- sample(1:n, 2/3*n)
train.set <- iris %>% slice(train.set.index) %>% arrange(Species)
test.set <- iris %>% slice(-train.set.index) %>% arrange(Species)
```


## Konstrukcja klasyfikatora i wyznaczenie prognoz

Stworzymy teraz macierze eksperymentu i wskaźnikową zarówno dla zbioru uczącego, jak i testowego. W tym celu wykorzystamy funkcję `dummyVars` z pakietu \verb|caret|.

```{r}
dummies <- dummyVars(" ~ .", data=iris)

train.dummies <- predict(dummies, newdata = train.set)
train.X <- as.matrix(cbind(rep(1, nrow(train.dummies)), 
                           train.dummies[, 1:4]))
train.Y <- train.dummies[, 5:7]

test.dummies <- predict(dummies, newdata = test.set) 
test.X <- as.matrix(cbind(rep(1, nrow(test.dummies)), test.dummies[, 1:4]))
test.Y <- test.dummies[, 5:7]
```

Wykorzystując metodę najmniejszych kwadratów, wyznaczamy przewidywane prognozy klas dla obu zbiorów.

```{r}
Y.hat <- solve(t(train.X) %*% train.X) %*% t(train.X) %*% train.Y

train.proba <- train.X %*% Y.hat
test.proba <- test.X %*% Y.hat
```

Przedstawmy prognozy klas na wykresach.

```{r train_plot, echo=FALSE, message=FALSE, fig.height=3, fig.cap="\\label{fig:fig1}Prognozy klas dla zbioru uczacego."}
train.plot <- melt(as.data.frame(train.proba))
train.plot$id <- as.integer(rownames(train.proba))
ggplot(train.plot, aes(x=id, y=value, color=variable)) +
  geom_point()
```

```{r test_plot, echo=FALSE, message=FALSE, fig.height=3, fig.cap="\\label{fig:fig2}Prognozy klas dla zbioru testowego."}
train.prediction <- colnames(train.proba)[apply(train.proba, 1, which.max)]
test.plot <- melt(as.data.frame(test.proba))
test.plot$id <- as.integer(rownames(test.proba))
ggplot(test.plot, aes(x=id, y=value, color=variable)) +
  geom_point()
```

\newpage

## Ocena jakości klasyfikacji

Wyznaczmy teraz macierz pomyłek dla zbioru uczącego.

```{r training_confusion_matrix, echo=FALSE, eval=TRUE, results='asis'}
train.confusion <- table(train.set$Species, train.prediction)

tab1 <- xtable(train.confusion, digits = 3, row.names = TRUE, caption = "Macierz pomylek dla zbioru uczacego.", label = "tab:tabela1")
print(tab1, type = "latex", table.placement = "H", comment=FALSE)
```

Błąd klasyfikacji to `r 1 - sum(diag(train.confusion)) / length(train.prediction)`.

Podobnie, wyznaczymy teraz macierz pomyłek dla zbioru testowego.

```{r testing_confusion_matrix, echo=FALSE, eval=TRUE, results='asis'}
test.prediction <- colnames(test.proba)[apply(test.proba, 1, which.max)]
test.confusion <- table(test.set$Species, test.prediction)

tab2 <- xtable(test.confusion, digits = 3, row.names = TRUE, caption = "Macierz pomylek dla zbioru testowego.", label = "tab:tabela2")
print(tab2, type = "latex", table.placement = "H", comment=FALSE)
```

Błąd klasyfikacji wynosi `r  1 - sum(diag(test.confusion)) / length(test.prediction)`.

Przypatując się wykresom (\ref{fig:fig1}), (\ref{fig:fig2}) możemy zauważyć, że zachodzi zjawisko maskowania --- klasa \verb|versicolor| jest przysłaniana.


## Zastosowanie regresji liniowej do modelu o rozszerzonej ilości cech

Najpierw uzupełnijmy dane o irysach o składniki wielomianowe stopnia $2$.

```{r}
iris.quad <- (iris %>% select(-Species))^2
colnames(iris.quad) <- c("SL^2", "SW^2", "PL^2", "PW^2")
iris <- cbind(iris, combn(iris %>% select(-Species), 2, 
                          FUN = Reduce, f = `*`), 
              iris.quad)
```

Podobnie jak poprzednio podzielimy dane na zbiory: uczący i testowy, a następnie utworzymy macierze: eksperymentu i indykatorów.

```{r}
train.set.index <- sample(1:n, 2/3*n)
train.set <- iris %>% slice(train.set.index) %>% arrange(Species)
test.set <- iris %>% slice(-train.set.index) %>% arrange(Species)

dummies <- dummyVars(" ~ .", data=iris)
train.dummies <- predict(dummies, newdata = train.set)
train.X <- as.matrix(cbind(rep(1, nrow(train.dummies)), train.dummies[, -c(5:7)]))
train.Y <- train.dummies[, 5:7]
test.dummies <- predict(dummies, newdata = test.set) 
test.X <- as.matrix(cbind(rep(1, nrow(test.dummies)), test.dummies[, -c(5:7)]))
test.Y <- test.dummies[, 5:7]
```

Ponownie, wyznaczymy prognozy klas i zwizualizujemy to przypisanie na wykresach.
```{r}
Y.hat <- solve(t(train.X) %*% train.X) %*% t(train.X) %*% train.Y

train.proba <- train.X %*% Y.hat
test.proba <- test.X %*% Y.hat
```

```{r train_plot_2, echo=FALSE, message=FALSE, fig.height=2.8, fig.cap="\\label{fig:fig3}Prognozy klas dla zbioru uczacego o rozszerzonej liczbie cech."}
train.plot <- melt(as.data.frame(train.proba))
train.plot$id <- as.integer(rownames(train.proba))
ggplot(train.plot, aes(x=id, y=value, color=variable)) +
  geom_point()
```

```{r test_plot_2, echo=FALSE, message=FALSE, fig.height=2.8, fig.cap="\\label{fig:fig4}Prognozy klas dla zbioru uczacego o rozszerzonej liczbie cech."}
test.plot <- melt(as.data.frame(test.proba))
test.plot$id <- as.integer(rownames(test.proba))
ggplot(test.plot, aes(x=id, y=value, color=variable)) +
  geom_point()
```

Wyznaczymy także macierze pomyłek i błędy klasyfikacji.

```{r training_confusion_matrix_2, echo=FALSE, eval=TRUE, results='asis'}
train.prediction <- colnames(train.proba)[apply(train.proba, 1, which.max)]
train.confusion <- table(train.set$Species, train.prediction)

tab3 <- xtable(train.confusion, digits = 3, row.names = TRUE, caption = "Macierz pomylek dla zbioru uczacego dla przypadku o rozszerzonej liczbie cech.", label = "tab:tabela3")
print(tab3, type = "latex", table.placement = "H", comment=FALSE)
```
Błąd klasyfkacji wynosi `r 1 - sum(diag(train.confusion)) / length(train.prediction)`. 


```{r testing_confusion_matrix_2, echo=FALSE, eval=TRUE, results='asis'}
test.prediction <- colnames(test.proba)[apply(test.proba, 1, which.max)]
test.confusion <- table(test.set$Species, test.prediction)

tab4 <- xtable(test.confusion, digits = 3, row.names = TRUE, caption = "Macierz pomylek dla zbioru testowego dla przypadku o rozszerzonej liczbie cech.", label = "tab:tabela4")
print(tab4, type = "latex", table.placement = "H", comment=FALSE)
```

Błąd klasyfikacji wynosi `r 1 - sum(diag(test.confusion)) / length(test.prediction)`.

Po przeanalizowaniu wykresów (\ref{fig:fig3}) i (\ref{fig:fig4}) dochodzimy do wniosku, że w tym przypadku zjawisko maskowania klas zostało zniwelowane.

## Wnioski

Przede wszystkim zauważamy, że model oparty na rozszerzonej ilości cech dał znacznie lepsze rezultaty --- błędy klasyfikacji były mniejsze zarówno dla zbioru uczącego, jak i testowego. W drugim modelu nie wystąpiło także zjawisko maskowania.

\newpage

# Zadanie 2

## Wczytanie i krótka anliza danych

Wczytajmy i zapoznajmy się z danymi.

```{r loading_and_preparing_data, message=FALSE}
data(wine)
n <- dim(wine)[1]
variable.number <- ncol(wine)
observations.number <- nrow(wine)
NaN.number <- sum(is.na(wine))
class.number <- length(unique(wine$Type))
```

Mamy `r variable.number` zmiennych i `r observations.number` obserwacji. Są trzy klasy, informację o nich zawiera zmienna \verb|Type|. Nie występują wartości brakujące (\verb|NaN.number| = `r NaN.number`).

Przyjrzyjmy się naszym danym na wykresach pudełkowych --- wykresy (\ref{box::1}) i (\ref{box::2}).

```{r Wine_boxplot_1, echo=FALSE, eval=TRUE, fig.cap="\\label{box::1}Wykresy pudelkowe naszych danych."}
plot_boxplot(wine[, c(1, 2:9)], by="Type")
```

```{r Wine_boxplot_2, echo=FALSE, eval=TRUE, fig.cap="\\label{box::2}Wykresy pudelkowe naszych danych.", fig.height=3}
plot_boxplot(wine[, c(1, 10:13)], by="Type")
```

Możemy zauważyć, że zmiennymi, które dobrze dywersyfikują klasy są: \verb|Alcohol|, \verb|Flavanoids| i  \verb|Phenols|.

Podzielimy nasze dane na zbiór uczący i testowy w stosunku $2:1$. Utworzymy także podzbiory, które będą zawierać tylko najlepiej dywersyfikujące cechy.

```{r learning and training test}
set.seed(42)
train.index <- sample(n, 2/3 * n)
train.data <- wine %>% slice(train.index)
test.data <- wine %>% slice(-train.index)
train.subset <- data.frame(train.data[, c(1, 2, 7, 8)])
test.subset <- data.frame(test.data[, c(1, 2, 7, 8)])
```

Zdefiniujmy też od razu rzeczywiste etykietki klas dla wcześniej utworzonych zbiorów.

```{r classes etiquettes}
train.etiquettes <- train.data$Type
test.etiquettes <- test.data$Type
subset.train.etiquettes <- train.subset$Type
subset.test.etiquettes <- test.subset$Type
```

Poniżej tworzymy także obiekt \verb|trainControl|, który wykorzystamy przy przeprowadzaniu $5$-krotnej walidacji krzyżowej. 

```{r tuning}
cv <- trainControl(method="cv", number=5)
```

## Metoda k-najbliższych sąsiadów 

Na początku wytrenujmy nasz klasyfikator na zbiorze uczącym zawierającym wszystkie cechy (przyjmujemy $k=5$).

```{r basic_knn}
model.knn.basic <- ipredknn(Type ~ ., data = train.data, k=5)

basic.knn.test.pred <- predict(model.knn.basic, test.data, type="class")
basic.knn.train.pred <- predict(model.knn.basic, train.data, type="class")
```

Wyznaczmy dla niego macierze pomyłek.

```{r echo=FALSE, eval=TRUE}
test.confusion <- table(basic.knn.test.pred, test.etiquettes)
train.confusion <- table(basic.knn.train.pred, train.etiquettes)

print(xtable(train.confusion), file="knn1.tex", floating=FALSE)
print(xtable(test.confusion), file="knn2.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabknn1}{\input{./knn1}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabknn2}{\input{./knn2}}}
\caption{Macierze pomylek dla metody KNN --- wszystkie cechy.}
\label{tab:tab_knn1}
\end{table}

Błędy klasyfikacji to `r 1 - sum(diag(train.confusion)) / length(subset.train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(subset.test.etiquettes)`.

Teraz stworzymy klasyfikator na zbiorze uczącym zawierającym wybrane cechy (przyjmujemy $k=5$).

```{r subset_knn}
knn.model.subset <- ipredknn(Type ~ ., data = train.subset, k=5)

subset.knn.test.pred <- predict(knn.model.subset, test.subset, type="class")
subset.knn.train.pred <- predict(knn.model.subset, train.subset, type="class")
```

Macierze pomyłek wyglądają następująco:

```{r echo=FALSE, eval=TRUE}
test.confusion <- table(subset.knn.test.pred, test.etiquettes)
train.confusion <- table(subset.knn.train.pred, train.etiquettes)

print(xtable(train.confusion), file="knn3.tex", floating=FALSE)
print(xtable(test.confusion), file="knn4.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabknn3}{\input{./knn3}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabknn4}{\input{./knn4}}}
\caption{Macierze pomylek dla metody KNN --- wybrance cechy.}
\label{tab:tab_knn2}
\end{table}

Błędy klasyfikacji to `r 1 - sum(diag(train.confusion)) / length(subset.train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(subset.test.etiquettes)`.

Widzimy, że klasyfikator wyćwiczony na \verb|train.subset| poradził sobie lepiej.
Zobaczmy więc jak zmienia się błąd klasyfikacji w zależności od parametru $k$ na zbiorze z ograniczoną liczbą cech.

\newpage

```{r k_comparison, echo=FALSE, eval=TRUE, fig.cap="Blad klasyfikacji w zaleznosci od parametru k."}
ks <- seq(20)
results <- data.frame(k=ks, train=rep(0, 20), test=rep(0, 20))

for (i in ks) {
  knn.model.subset <- ipredknn(Type ~ ., data = train.subset, k=i)
  subset.knn.test.pred <- predict(knn.model.subset, test.subset, type="class")
  subset.knn.train.pred <- predict(knn.model.subset, train.subset, type="class")
  test.confusion <- table(subset.knn.test.pred, test.etiquettes)
  train.confusion <- table(subset.knn.train.pred, train.etiquettes)
  results[i, 2] = 1 - sum(diag(train.confusion)) / length(subset.train.etiquettes)
  results[i, 3] = 1 - sum(diag(test.confusion)) / length(subset.test.etiquettes)
}

ggplot(data = melt(results, id.vars = "k", variable.name = "set")) + 
  geom_point(aes(x=k, y=value, color=set)) + geom_line(aes(x=k, y=value, color=set))
```

Widzimy, że najlepsze rezultaty otrzymujemy dla $k=9$. 

Wykorzystamy teraz pakiet \verb|caret| do stworzenia modelu stuningowanego. By taki model powstał, wykorzystamy $5$-krotną walidację krzyżową.

```{r tuned_knn}
model <- train(Type ~ ., data = train.subset, method = "knn", trControl = cv)

tuned.knn.test.pred <- predict(model, test.data)
tuned.knn.train.pred <- predict(model, train.data)
```

Jak się okazuje, model ten również przyjmuje $k = $ `r model$bestTune`.

Dla tego klasyfikatora także wyznaczymy macierze pomyłek i błędy klasyfikacji.

```{r echo=FALSE, eval=TRUE}
test.confusion <- table(tuned.knn.test.pred, test.etiquettes)
train.confusion <- table(tuned.knn.train.pred, train.etiquettes)

print(xtable(train.confusion), file="knn5.tex", floating=FALSE)
print(xtable(test.confusion), file="knn6.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabknn5}{\input{./knn5}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabknn6}{\input{./knn6}}}
\caption{Macierze pomylek dla metody KNN --- stuningowany model.}
\label{tab:tab_knn3}
\end{table}

Błędy klasyfikacji to `r 1 - sum(diag(train.confusion)) / length(subset.train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(subset.test.etiquettes)`.

Jak widzimy model stuningowany poradził sobie najlepiej. Wyznaczymy teraz dla niego błąd predykcji --- skorzystamy z $5$-krotnej walidacji krzyżowej, metody bootstrap oraz \verb|.632+|.

```{r}
predictor  <- function(model, newdata) 
{ predict(model, newdata=newdata) }

knn.predictor <- function(formula, data) 
{ train(formula, data = data, method = "knn", trControl = cv)} 

knn.error.cv <- errorest(Type~., wine[, c(1, 2, 7, 8)], model=knn.predictor, 
                         predict=predictor, estimator="cv", 
                         est.para=control.errorest(k = 5))

knn.error.boot <- errorest(Type~., wine[, c(1, 2, 7, 8)], model=knn.predictor, 
                         predict=predictor, estimator="boot", 
                         est.para=control.errorest(nboot = 25))

knn.error.632 <- errorest(Type~., wine[, c(1, 2, 7, 8)], model=knn.predictor, 
                         predict=predictor, estimator="632plus", 
                         est.para=control.errorest(nboot = 25))
```

Błędy wyniosły kolejno `r knn.error.cv$error`, `r knn.error.boot$error` oraz `r knn.error.632$error`.


```{r}
library(klaR)
drawparti(wine$Type, x=wine$Alcohol, y=wine$Flavanoids, method = "sknn",
          plot.matrix = FALSE, imageplot = TRUE, col.contour = "red",
          col.wrong = "red", image.colors = wes_palette("GrandBudapest1", 3),
          col.correct = "black", xlab="Alcohol", ylab="Flavanoids")
```

```{r}
model <- train(Type ~ Alcohol + Flavanoids, data = train.subset, method = "knn", trControl = cv)

tuned.knn.test.pred <- predict(model, test.data)
tuned.knn.train.pred <- predict(model, train.data)

tuned.predictions <- predict(model, test.subset)

var1 <- wine$Alcohol
var2 <- wine$Flavanoids

var3 <- test.subset$Alcohol
var4 <- test.subset$Flavanoids

tuned.predictions <- predict(model, test.subset)

test.confusion <- table(tuned.predictions, test.etiquettes)

x.test <- expand.grid(Alcohol = seq(min(var1), max(var1), by = 0.1), 
                      Flavanoids = seq(min(var2), max(var2), by=0.1))

x.test.pred <- predict(model, x.test)

df1 <- data.frame(x.test, class = x.test.pred)
df2 <- data.frame(x = var3, y = var4, class = tuned.predictions)

ggplot() +
  geom_point(aes(x=Alcohol, y=Flavanoids, col = class), data = df1, size = 1) + 
  geom_point(aes(x = x, y = y, col = class), data = df2, size = 4.5, shape = 1) + 
  theme_bw() 
```

## Drzewa klasyfikacyjne

Najpierw wytrenujemy model na zbiorze uczącym zawierającym wszystkie cechy.

```{r basic_tree_model}
basic.tree.model <- rpart(Type ~ ., data = train.data)

basic.tree.test.pred <- predict(basic.tree.model, newdata = test.data, 
                                type = "class")
basic.tree.train.pred <- predict(basic.tree.model, newdata = train.data, 
                                 type = "class")
```

To drzewo klasyfikacyjne wygląda następująco (\ref{fig:fig5}).

```{r basic_tree_plot, echo=FALSE, eval=TRUE, fig.cap="\\label{fig:fig5}Drzewo decyzyjne --- wszystkie cechy."}
fancyRpartPlot(basic.tree.model, sub="")
```

Wyznaczymy dla tego modelu macierze pomyłek i błędy klasyfikacji.

```{r echo=FALSE, eval=TRUE}
test.confusion <- table(basic.tree.test.pred, test.etiquettes)
train.confusion <- table(basic.tree.train.pred, train.etiquettes)

print(xtable(train.confusion), file="t1.tex", floating=FALSE)
print(xtable(test.confusion), file="t2.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabt1}{\input{./t1}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabt2}{\input{./t2}}}
\caption{Macierze pomylek dla metody drzew klasyfikacyjnych --- wszystkie cechy.}
\label{tab:tab_t1}
\end{table}

Błędy klasyfikacji to `r 1 - sum(diag(train.confusion)) / length(subset.train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(subset.test.etiquettes)`.


Teraz stworzymy model, który wytrenujemy na \verb|train.subset|.

```{r subset_tree_model}
subset.tree.model <- rpart(Type ~ ., data = train.subset)

subset.tree.test.pred <- predict(subset.tree.model, newdata = test.subset, 
                                 type = "class")
subset.tree.train.pred <- predict(subset.tree.model, newdata = train.subset,
                                  type = "class")
```

To drzewo decyzyjne wygląda następująco (\ref{fig:fig6}).

```{r subset_tree_plot, echo=FALSE, eval=TRUE, fig.cap="\\label{fig:fig6} Drzewo decyzyjne --- wybrane cechy."}
fancyRpartPlot(subset.tree.model, sub="")
```

Podobnie jak wczęśniej, wyznaczymy dla niego macierze pomyłek i błędy klasyfikacji.

```{r echo=FALSE, eval=TRUE}
test.confusion <- table(subset.tree.test.pred, subset.test.etiquettes)
train.confusion <- table(subset.tree.train.pred, subset.train.etiquettes)

print(xtable(train.confusion), file="t3.tex", floating=FALSE)
print(xtable(test.confusion), file="t4.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabt3}{\input{./t3}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabt4}{\input{./t4}}}
\caption{Macierze pomylek dla metody drzew klasyfikacyjnych --- wybrane cechy.}
\label{tab:tab_t2}
\end{table}

Błędy klasyfikacji to `r 1 - sum(diag(train.confusion)) / length(subset.train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(subset.test.etiquettes)`.


Ponownie stworzymy stuningowany model, zmieniając parametr \verb|cp|, przy pomocy 5-krotnej walidacji krzyżowej. 

```{r}
tuned.tree.model <- train(Type ~ ., data = train.subset, method = "rpart",
                          trControl = cv, tuneLength = 10)

tuned.tree.test.pred <- predict(tuned.tree.model, test.subset)
tuned.tree.train.pred <- predict(tuned.tree.model, train.subset)
```


```{r echo=FALSE, eval=TRUE, fig.cap="Drzewo decyzyjne --- stunigowany model."}
fancyRpartPlot(tuned.tree.model$finalModel, sub="")
```


```{r echo=FALSE, eval=TRUE}
test.confusion <- table(tuned.tree.test.pred, test.etiquettes)
train.confusion <- table(tuned.tree.train.pred, train.etiquettes)

print(xtable(train.confusion), file="t5.tex", floating=FALSE)
print(xtable(test.confusion), file="t6.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabt5}{\input{./t5}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabt6}{\input{./t6}}}
\caption{Macierze pomylek dla metody drzew klasyfikacyjnych --- stuningowany model.}
\label{tab:tab_t3}
\end{table}

Błędy klasyfikacji to `r 1 - sum(diag(train.confusion)) / length(subset.train.etiquettes) ` i `r 1 - sum(diag(test.confusion)) / length(subset.test.etiquettes)`.


Zobaczmy jak wpłynie na model wytrenowany na \verb|train.subset| zmiana parametrów drzewa --- tabela (\ref{tab:tab101}).

```{r message=FALSE, warning=FALSE}
search_grid <- expand.grid(cp=seq(0,0.1,0.001))

best.model.tree <- train(Type ~., data = train.subset, method = "rpart",
                          trControl = cv, tuneGrid = search_grid)

a  <- best.model.tree$results %>% top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))
```


```{r results='asis', eval=TRUE, echo=FALSE}
results <- xtable(a[1:5, ], digits = 3, row.names = TRUE, caption = "5 najlepszych modeli drzewa decyzyjnego.", label = "tab:tab101")
print(results, type = "latex", table.placement = "H", comment=FALSE)
```

Widzimy, że najlepsze reultaty otrzymujemy dla \verb|cp| = $0$ --- drzewa nie trzeba w ogóle przycinać.

Jak widzimy model stuningowany poradził sobie najlepiej. Wyznaczymy teraz dla niego błąd predykcji --- skorzystamy ponownie z $5$-krotnej walidacji krzyżowej, metody bootstrap oraz \verb|.632+|.

```{r}
predictor  <- function(model, newdata) 
{ predict(model, newdata=newdata, type = "class") }

decision.tree.predictor <- function(formula, data) 
{ rpart(formula, data = data, cp = 0)}

decision.tree.error.cv <- errorest(Type~., wine[, c(1, 2, 7, 8)], 
                                   model=decision.tree.predictor, 
                                   predict=predictor, estimator="cv", 
                                   est.para=control.errorest(k = 5))

decision.tree.error.boot <- errorest(Type~., wine[, c(1, 2, 7, 8)], 
                                     model=decision.tree.predictor, 
                                     predict=predictor, estimator="boot", 
                                     est.para=control.errorest(nboot = 25))

decision.tree.error.632 <- errorest(Type~., wine[, c(1, 2, 7, 8)], 
                                     model=decision.tree.predictor, 
                                     predict=predictor, estimator="632plus", 
                                     est.para=control.errorest(nboot = 25))
```

Błędy wyniosły kolejno `r decision.tree.error.cv$error`, `r decision.tree.error.boot$error` oraz `r decision.tree.error.632$error`.

## Naiwny klasyfikator bayesowski

Najpierw wytrenujemy model na zbiorze uczącym zawierającym wszystkie zmienne.

```{r basic_bayes}
bayes.model.basic <- naiveBayes(Type ~ ., data = train.data)

basic.bayes.train.pred <- predict(bayes.model.basic, train.data) 
basic.bayes.test.pred <- predict(bayes.model.basic, test.data)
```

Wyznaczmy dla tego modelu macierze pomyłek i wartości błędów klasyfikacji.

```{r basic_bayes_tables, echo=FALSE, eval=TRUE}
test.confusion <- table(test.etiquettes, basic.bayes.test.pred)
train.confusion <- table(train.etiquettes, basic.bayes.train.pred)

print(xtable(train.confusion), file="b1.tex", floating=FALSE)
print(xtable(test.confusion), file="b2.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabb1}{\input{./b1}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabb2}{\input{./b2}}}
\caption{Macierze pomylek dla klasyfikatora bayesowskiego --- wszystkie cechy.}
\label{tab:tab_b1}
\end{table}

Błędy klasyfikacji to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes)` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.


Powtórzmy teraz powyższe dla wybranego podzbioru naszych danych.

```{r subset_bayes}
bayes.model.subset <- naiveBayes(Type ~ ., data = train.subset)

bayes.subset.train.pred <- predict(bayes.model.subset, train.subset) 
bayes.subset.test.pred <- predict(bayes.model.subset, test.subset)
```


```{r subset_bayes_tables, echo=FALSE, eval=TRUE}
test.confusion <- table(subset.test.etiquettes, bayes.subset.test.pred)
train.confusion <- table(subset.train.etiquettes, bayes.subset.train.pred)

print(xtable(train.confusion), file="b3.tex", floating=FALSE)
print(xtable(test.confusion), file="b4.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabb3}{\input{./b3}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabb4}{\input{./b4}}}
\caption{Macierze pomylek dla klasyfikatora bayesowskiego --- wybrane cechy.}
\label{tab:tab_b2}
\end{table}

Błędy klasyfikacji to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes)` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.

Ponownie skorzystamy z pakietu \verb|caret|, by stworzyć model stunigowany, wytrenowany na wsszystkich cechach. Wyznaczymy dla niego macierze pomyłek i błędy klasyfikacji.

```{r}
model <- train(Type ~ ., data = train.data, method = "naive_bayes", 
               trControl = cv)
```


```{r echo=FALSE, eval=TRUE}
test.confusion <- table(test.data$Type, predict(model, test.data))
train.confusion <- table(train.data$Type, predict(model, train.data))

print(xtable(train.confusion), file="b5.tex", floating=FALSE)
print(xtable(test.confusion), file="b6.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabb5}{\input{./b5}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabb6}{\input{./b6}}}
\caption{Macierze pomylek dla klasyfikatora bayesowskiego --- model stuningowany.}
\label{tab:tab_b3}
\end{table}

Błędy klasyfikacji w tym przypadku to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes)` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.

Widzimy, że poradził on sobie najlepiej z trzech rozważanych modeli.  

Powtórzymy teraz ocenę klasyfikacji, podobnie jak dla wcześniej, dla stuningowanego klasyfikatora bayesowskiego.

```{r}
predictor  <- function(model, newdata) 
{ predict(model, newdata=newdata) }
 
naiveBayes.predictor <- function(formula, data) 
{ train(formula, data = data, method = "naive_bayes", trControl = cv)} 
 
naiveBayes.error.cv <- errorest(Type~., wine, model=naiveBayes.predictor, 
                                   predict=predictor, 
                                   estimator="cv", 
                                   est.para=control.errorest(k = 5))

naiveBayes.error.boot <- errorest(Type~., wine, model=naiveBayes.predictor, 
                                     predict=predictor, estimator="boot", 
                                     est.para=control.errorest(nboot = 25))

naiveBayes.error.632 <- errorest(Type~., wine, model=naiveBayes.predictor, 
                                     predict=predictor, estimator="632plus", 
                                     est.para=control.errorest(nboot = 25))
```

Błędy predykcji wyniosły kolejno `r naiveBayes.error.cv$error`, `r naiveBayes.error.boot$error` oraz `r naiveBayes.error.632$error`. 

## Wieloklasowa regresja logistyczna 

Najpierw wytrenujemy model na zbiorze uczącym zawierającym wszystkie zmienne.

```{r mlr_basic, message=FALSE, results = "hide"}
mlr.model.basic <- multinom(Type ~ ., data = train.data)

basic.mlr.train.pred <- predict(mlr.model.basic, newdata = train.data, type = "class") 
basic.mlr.test.pred <- predict(mlr.model.basic, newdata = test.data, type = "class")
```

Wyznaczmy dla tego modelu macierze pomyłek i wartości błędów klasyfikacji.

```{r basic_mlr_tables, echo=FALSE, eval=TRUE}
test.confusion <- table(test.etiquettes, basic.mlr.test.pred)
train.confusion <- table(train.etiquettes, basic.mlr.train.pred)

print(xtable(train.confusion), file="mlr1.tex", floating=FALSE)
print(xtable(test.confusion), file="mlr2.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabmlr1}{\input{./mlr1}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabmlr2}{\input{./mlr2}}}
\caption{Macierze pomylek dla regresji wieloklasowej --- wszystkie cechy.}
\label{tab:tab_mlr1}
\end{table}

Błędy klasyfikacji to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes)` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.


Powtórzmy teraz powyższe dla wybranego podzbioru naszych danych.

```{r subset_mlr, message=FALSE, results = "hide"}
mlr.model.subset <- multinom(Type ~ ., data = train.subset)

mlr.subset.train.pred <- predict(mlr.model.subset, train.subset, type = "class") 
mlr.subset.test.pred <- predict(mlr.model.subset, test.subset, type = "class")
```


```{r subset_mlr_tables, echo=FALSE, eval=TRUE}
test.confusion <- table(subset.test.etiquettes, mlr.subset.test.pred)
train.confusion <- table(subset.train.etiquettes, mlr.subset.train.pred)

print(xtable(train.confusion), file="mlr3.tex", floating=FALSE)
print(xtable(test.confusion), file="mlr4.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabmlr3}{\input{./mlr3}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabmlr4}{\input{./mlr4}}}
\caption{Macierze pomylek dla wieloklasowej regresji --- wybrane cechy.}
\label{tab:tab_mlr2}
\end{table}

Błędy klasyfikacji to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes)` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.

Ponownie skorzystamy z pakietu \verb|caret|, by stworzyć model stunigowany. Wyznaczymy dla niego macierze pomyłek i błędy klasyfikacji.

```{r tuned_mlr, message=FALSE, results = "hide"}
model <- train(Type ~ ., data = train.data, method = "multinom", 
               trControl = cv)
```


```{r echo=FALSE, eval=TRUE}
test.confusion <- table(test.data$Type, predict(model, test.data))
train.confusion <- table(train.data$Type, predict(model, train.data))

print(xtable(train.confusion), file="mlr5.tex", floating=FALSE)
print(xtable(test.confusion), file="mlr6.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Zbior uczacy]{\label{tab:tabmlr5}{\input{./mlr5}}}\quad
\subfloat[Zbior testowy]{\label{tab:tabmlr6}{\input{./mlr6}}}
\caption{Macierze pomylek dla klasyfikatora regresji wieloklasowej --- model stuningowany.}
\label{tab:tab_mlr3}
\end{table}

Błędy klasyfikacji w tym przypadku to kolejno `r 1 - sum(diag(train.confusion)) / length(train.etiquettes)` i `r 1 - sum(diag(test.confusion)) / length(test.etiquettes)`.

Powtórzymy teraz podobną jak wcześniej ocenę klasyfikacji dla stuningowanego modelu regresji wieloklasowej.

```{r cross_validation_mlr, message=FALSE, results = "hide"}
predictor  <- function(model, newdata) 
{ predict(model, newdata=newdata) }
 
mlr.predictor <- function(formula, data) 
{ train(formula, data = data, method = "multinom", trControl = cv)} 
 
mlr.error.cv <- errorest(Type~., wine, model=mlr.predictor, predict=predictor, 
            estimator="cv", est.para=control.errorest(k = 5))

mlr.error.boot <- errorest(Type~., wine[, c(1, 2, 7, 8)], 
                                     model=mlr.predictor, 
                                     predict=predictor, estimator="boot", 
                                     est.para=control.errorest(nboot = 25))

mlr.error.632 <- errorest(Type~., wine[, c(1, 2, 7, 8)], 
                                     model=mlr.predictor, 
                                     predict=predictor, estimator="632plus", 
                                     est.para=control.errorest(nboot = 25))
```

Błędy predykcji wyniosły `r mlr.error.cv$error`, `r mlr.error.boot$error` oraz `r mlr.error.632$error`. 

## Podsumowanie

Porównaliśmy ze sobą 4 klasyfikatory. Dla każdego z nich badaliśmy jaki wpływ na dokładność ich predykcji ma zmiana charaktrystycznych dla nich parametrów czy zmiany w zbiorze uczącym (klasyfikatory trenowliśmy na zbiorach, które albo zawierały wszystkie zmienne, albo te wybrane przez nas, które wprowadzły najlepszy podział na zmienne).

\begin{itemize}
  \item Dla metody $k$ najbliższych sąsiadów najlepsze rezultaty otrzymaliśmy dla $k = 9$ i zbioru uczącego o zawężonej liczbie cech.
  \item Dla metody drzew decyzyjnych najlepszy okazała się wartość parametru $cp = 0$ i wyuczenie modelu na zbiorze z wybranymi przez nas cechami.
  \item Dla naiwnego klasyfikatora bayesowskiego najlepsze efekty otrzymaliśmy, gdy wyuczyliśmy model na zbiorze uczącym zawierającym wszystkie zmienne.
  \item Dla wieloklasowej regresji logistycznej najefektywniejsze okazało się wyuczenie modelu na zbiorze zawierającym wszystkie zmienne.
  
\end{itemize}

Błędy predykacji dla $5$-krotnej walidacji krzyżowej, metody bootstrap oraz \verb|.632+| wyglądają następująco --- patrz tabela. 


\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\hline
Metoda & KNN & Drzewa decyzyjne & Naiwny klasyfikator bayesowski & Wieloklasowa regresja logistyczna \\
\hline
CV & `r knn.error.cv$error` & `r decision.tree.error.cv$error` & `r naiveBayes.error.cv$error` & `r mlr.error.cv$error` \\
Bootstrap & `r knn.error.boot$error` & `r decision.tree.error.boot$error` & `r naiveBayes.error.boot$error` & `r mlr.error.boot$error`\\
632+ & `r knn.error.632$error` & `r decision.tree.error.632$error` & `r naiveBayes.error.632$error` & `r mlr.error.632$error`\\
\hline
\end{tabular}
}
\caption{Wartości błędów predykcji.}
\label{tab:tab_last}
\end{table}


Możemy zauważyć, że najlepszą metodą okazał się naiwny klasyfikator bayesowski. Najgorzej natomiast poradziły sobie drzewa decyzyjne. 
